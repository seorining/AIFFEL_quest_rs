{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab 객체 생성\n",
    "mecab = Mecab()\n",
    "\n",
    "# 데이터 읽기\n",
    "with open(\"korean-english-park.train.ko\", \"r\", encoding=\"utf-8\") as f:\n",
    "    kor_lines = f.readlines()\n",
    "\n",
    "with open(\"korean-english-park.train.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eng_lines = f.readlines()\n",
    "\n",
    "# 한글 전처리\n",
    "def preprocess_kor(text):\n",
    "    text = re.sub(r\"[^ㄱ-ㅎ가-힣0-9\\s]\", \"\", text)  # 한글, 숫자, 공백만 남김\n",
    "    return mecab.morphs(text)  # 형태소 분석\n",
    "\n",
    "# 영어 전처리\n",
    "def preprocess_eng(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text.lower())  # 영어, 숫자, 공백만 남김\n",
    "    return [\"<start>\"] + text.split() + [\"<end>\"]  # 토큰화\n",
    "\n",
    "\n",
    "# 병렬 데이터 정제 및 중복 제거\n",
    "cleaned_corpus = set()\n",
    "for kor, eng in zip(kor_lines, eng_lines):\n",
    "    kor_tokens = preprocess_kor(kor.strip())\n",
    "    eng_tokens = preprocess_eng(eng.strip())\n",
    "    if len(kor_tokens) <= 40 and len(eng_tokens) <= 40:\n",
    "        cleaned_corpus.add((tuple(kor_tokens), tuple(eng_tokens)))\n",
    "\n",
    "# 병렬 데이터 리스트 변환\n",
    "cleaned_corpus = list(cleaned_corpus)\n",
    "\n",
    "# 병렬 데이터 분리\n",
    "kor_corpus = [\" \".join(kor) for kor, _ in cleaned_corpus]\n",
    "eng_corpus = [\" \".join(eng) for _, eng in cleaned_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: <start> the environmental consequences would be regional but the social and economic consequences would be global <end>\n",
      "Spanish: 환경 적 인 영향 은 지역 적 이 겠 지만 사회 및 경제 적 인 영향 은 전 세계 에 미치 게 될 겁니다\n"
     ]
    }
   ],
   "source": [
    "print(\"English:\", eng_corpus[100])\n",
    "print(\"Spanish:\", kor_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 09:53:22.924095: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-11 09:53:22.944596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-11 09:53:22.944614: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-11 09:53:22.944627: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-11 09:53:22.948565: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize(corpus, vocab_size=10000):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, filters=\"\", oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = pad_sequences(tensor, padding=\"post\")  # 패딩 추가\n",
    "    return tensor, tokenizer\n",
    "\n",
    "\n",
    "# 데이터 토큰화\n",
    "kor_tensor, kor_tokenizer = tokenize(kor_corpus, vocab_size=20000)\n",
    "eng_tensor, eng_tokenizer = tokenize(eng_corpus, vocab_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(enc_units, return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "    def call(self, x, initial_state):\n",
    "        x = self.embedding(x)\n",
    "        output, h, c = self.lstm(x, initial_state=initial_state)\n",
    "        return output, h, c\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            hidden[0], enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, h, c = self.lstm(x)\n",
    "        x = self.fc(output)\n",
    "        return x, h, c, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 09:54:52.071338: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-01-11 09:54:52.071349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: 11be54d0c487\n",
      "2025-01-11 09:54:52.071351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: 11be54d0c487\n",
      "2025-01-11 09:54:52.071374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.183.1\n",
      "2025-01-11 09:54:52.071379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.183.1\n",
      "2025-01-11 09:54:52.071380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.183.1\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# 손실 함수 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\")\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # 패딩 제외\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512\n",
    "batch_size = 64\n",
    "vocab_inp_size = len(kor_tokenizer.word_index) + 1\n",
    "vocab_tar_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "# 모델 초기화\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 루프\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp, enc_hidden)\n",
    "        dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "        dec_input = tf.expand_dims(\n",
    "            [eng_tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden_h, dec_hidden_c, _ = decoder(\n",
    "                dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = loss / int(targ.shape[1])\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.395641803741455\n",
      "Epoch 1 Batch 10 Loss 4.617795467376709\n",
      "Epoch 1 Batch 20 Loss 3.3545758724212646\n",
      "Epoch 1 Batch 30 Loss 3.4984796047210693\n",
      "Epoch 1 Batch 40 Loss 3.392894744873047\n",
      "Epoch 1 Batch 50 Loss 3.289454698562622\n",
      "Epoch 1 Batch 60 Loss 3.0413544178009033\n",
      "Epoch 1 Batch 70 Loss 3.175771713256836\n",
      "Epoch 1 Batch 80 Loss 3.5031380653381348\n",
      "Epoch 1 Batch 90 Loss 3.1396377086639404\n",
      "Epoch 1 Batch 100 Loss 3.352759599685669\n",
      "Epoch 1 Batch 110 Loss 3.335084915161133\n",
      "Epoch 1 Batch 120 Loss 3.2961952686309814\n",
      "Epoch 1 Batch 130 Loss 3.4688851833343506\n",
      "Epoch 1 Batch 140 Loss 3.2198104858398438\n",
      "Epoch 1 Batch 150 Loss 3.514211654663086\n",
      "Epoch 1 Batch 160 Loss 2.956819534301758\n",
      "Epoch 1 Batch 170 Loss 2.9151599407196045\n",
      "Epoch 1 Batch 180 Loss 3.126091241836548\n",
      "Epoch 1 Batch 190 Loss 3.133451461791992\n",
      "Epoch 1 Batch 200 Loss 3.436023712158203\n",
      "Epoch 1 Batch 210 Loss 3.1658456325531006\n",
      "Epoch 1 Batch 220 Loss 3.500621795654297\n",
      "Epoch 1 Batch 230 Loss 3.0166022777557373\n",
      "Epoch 1 Batch 240 Loss 3.8438007831573486\n",
      "Epoch 1 Batch 250 Loss 3.5604515075683594\n",
      "Epoch 1 Batch 260 Loss 3.1317648887634277\n",
      "Epoch 1 Batch 270 Loss 2.6740500926971436\n",
      "Epoch 1 Batch 280 Loss 3.230524778366089\n",
      "Epoch 1 Batch 290 Loss 3.063802480697632\n",
      "Epoch 1 Batch 300 Loss 2.951972484588623\n",
      "Epoch 1 Batch 310 Loss 3.2468483448028564\n",
      "Epoch 1 Batch 320 Loss 2.767261266708374\n",
      "Epoch 1 Batch 330 Loss 2.7709381580352783\n",
      "Epoch 1 Batch 340 Loss 3.218190908432007\n",
      "Epoch 1 Batch 350 Loss 2.80416202545166\n",
      "Epoch 1 Batch 360 Loss 2.8139617443084717\n",
      "Epoch 1 Batch 370 Loss 2.765003204345703\n",
      "Epoch 1 Batch 380 Loss 2.8118391036987305\n",
      "Epoch 1 Batch 390 Loss 2.7757155895233154\n",
      "Epoch 1 Batch 400 Loss 2.9519360065460205\n",
      "Epoch 1 Batch 410 Loss 2.8665695190429688\n",
      "Epoch 1 Batch 420 Loss 3.1378657817840576\n",
      "Epoch 1 Batch 430 Loss 2.9092037677764893\n",
      "Epoch 1 Batch 440 Loss 2.679924964904785\n",
      "Epoch 1 Batch 450 Loss 2.946507453918457\n",
      "Epoch 1 Batch 460 Loss 2.943052053451538\n",
      "Epoch 1 Batch 470 Loss 2.79699969291687\n",
      "Epoch 1 Batch 480 Loss 2.552828550338745\n",
      "Epoch 1 Batch 490 Loss 2.619260549545288\n",
      "Epoch 1 Batch 500 Loss 2.8547756671905518\n",
      "Epoch 1 Batch 510 Loss 2.697146415710449\n",
      "Epoch 1 Batch 520 Loss 2.612809896469116\n",
      "Epoch 1 Batch 530 Loss 2.8310182094573975\n",
      "Epoch 1 Batch 540 Loss 2.804624557495117\n",
      "Epoch 1 Batch 550 Loss 2.759896993637085\n",
      "Epoch 1 Batch 560 Loss 2.76606822013855\n",
      "Epoch 1 Batch 570 Loss 2.774904251098633\n",
      "Epoch 1 Batch 580 Loss 2.637622117996216\n",
      "Epoch 1 Batch 590 Loss 2.6166954040527344\n",
      "Epoch 1 Batch 600 Loss 2.8550477027893066\n",
      "Epoch 1 Batch 610 Loss 2.519280195236206\n",
      "Epoch 1 Batch 620 Loss 2.5689430236816406\n",
      "Epoch 1 Batch 630 Loss 2.8786799907684326\n",
      "Epoch 1 Batch 640 Loss 2.7013936042785645\n",
      "Epoch 1 Batch 650 Loss 2.5020711421966553\n",
      "Epoch 1 Batch 660 Loss 2.709620714187622\n",
      "Epoch 1 Batch 670 Loss 2.808905839920044\n",
      "Epoch 1 Batch 680 Loss 2.757399320602417\n",
      "Epoch 1 Batch 690 Loss 2.5610461235046387\n",
      "Epoch 1 Batch 700 Loss 2.529658079147339\n",
      "Epoch 1 Batch 710 Loss 2.6357295513153076\n",
      "Epoch 1 Batch 720 Loss 2.6677005290985107\n",
      "Epoch 1 Batch 730 Loss 2.7833335399627686\n",
      "Epoch 1 Batch 740 Loss 2.898726224899292\n",
      "Epoch 1 Batch 750 Loss 2.507397413253784\n",
      "Epoch 1 Batch 760 Loss 2.4097816944122314\n",
      "Epoch 1 Batch 770 Loss 2.867281436920166\n",
      "Epoch 1 Batch 780 Loss 2.463648557662964\n",
      "Epoch 1 Batch 790 Loss 2.4206600189208984\n",
      "Epoch 1 Batch 800 Loss 2.8956379890441895\n",
      "Epoch 1 Batch 810 Loss 2.5920941829681396\n",
      "Epoch 1 Batch 820 Loss 2.65482497215271\n",
      "Epoch 1 Batch 830 Loss 2.6066784858703613\n",
      "Epoch 1 Batch 840 Loss 2.7789642810821533\n",
      "Epoch 1 Batch 850 Loss 2.2213878631591797\n",
      "Epoch 1 Batch 860 Loss 2.93155837059021\n",
      "Epoch 1 Batch 870 Loss 2.6159024238586426\n",
      "Epoch 1 Batch 880 Loss 2.3874166011810303\n",
      "Epoch 1 Batch 890 Loss 2.5902822017669678\n",
      "Epoch 1 Batch 900 Loss 2.5383241176605225\n",
      "Epoch 1 Batch 910 Loss 2.6655538082122803\n",
      "Epoch 1 Batch 920 Loss 2.953630208969116\n",
      "Epoch 1 Batch 930 Loss 2.2364392280578613\n",
      "Epoch 1 Batch 940 Loss 2.7874603271484375\n",
      "Epoch 1 Batch 950 Loss 2.756810188293457\n",
      "Epoch 1 Batch 960 Loss 2.5027031898498535\n",
      "Epoch 1 Batch 970 Loss 2.9693565368652344\n",
      "Epoch 1 Batch 980 Loss 2.520718812942505\n",
      "Epoch 1 Batch 990 Loss 2.376729965209961\n",
      "Epoch 1 Batch 1000 Loss 2.674058198928833\n",
      "Epoch 1 Batch 1010 Loss 2.435986042022705\n",
      "Epoch 1 Batch 1020 Loss 2.551154851913452\n",
      "Epoch 1 Batch 1030 Loss 2.6814992427825928\n",
      "Epoch 1 Loss 2.8982\n",
      "Epoch 2 Batch 0 Loss 2.726081371307373\n",
      "Epoch 2 Batch 10 Loss 2.6858632564544678\n",
      "Epoch 2 Batch 20 Loss 2.455007553100586\n",
      "Epoch 2 Batch 30 Loss 2.3629512786865234\n",
      "Epoch 2 Batch 40 Loss 2.699202060699463\n",
      "Epoch 2 Batch 50 Loss 2.835031270980835\n",
      "Epoch 2 Batch 60 Loss 2.44307017326355\n",
      "Epoch 2 Batch 70 Loss 2.4554522037506104\n",
      "Epoch 2 Batch 80 Loss 2.514906644821167\n",
      "Epoch 2 Batch 90 Loss 2.555920124053955\n",
      "Epoch 2 Batch 100 Loss 2.953378438949585\n",
      "Epoch 2 Batch 110 Loss 2.5164408683776855\n",
      "Epoch 2 Batch 120 Loss 2.4226911067962646\n",
      "Epoch 2 Batch 130 Loss 2.5854668617248535\n",
      "Epoch 2 Batch 140 Loss 2.842618465423584\n",
      "Epoch 2 Batch 150 Loss 2.5737602710723877\n",
      "Epoch 2 Batch 160 Loss 2.6091957092285156\n",
      "Epoch 2 Batch 170 Loss 2.620022773742676\n",
      "Epoch 2 Batch 180 Loss 2.262845993041992\n",
      "Epoch 2 Batch 190 Loss 2.644275665283203\n",
      "Epoch 2 Batch 200 Loss 2.6285412311553955\n",
      "Epoch 2 Batch 210 Loss 2.5524251461029053\n",
      "Epoch 2 Batch 220 Loss 2.391343832015991\n",
      "Epoch 2 Batch 230 Loss 2.869978666305542\n",
      "Epoch 2 Batch 240 Loss 2.8961586952209473\n",
      "Epoch 2 Batch 250 Loss 2.495781183242798\n",
      "Epoch 2 Batch 260 Loss 2.47015643119812\n",
      "Epoch 2 Batch 270 Loss 2.4162187576293945\n",
      "Epoch 2 Batch 280 Loss 2.763920307159424\n",
      "Epoch 2 Batch 290 Loss 2.7519233226776123\n",
      "Epoch 2 Batch 300 Loss 2.336336851119995\n",
      "Epoch 2 Batch 310 Loss 2.391178607940674\n",
      "Epoch 2 Batch 320 Loss 2.5951387882232666\n",
      "Epoch 2 Batch 330 Loss 2.479905605316162\n",
      "Epoch 2 Batch 340 Loss 2.4847145080566406\n",
      "Epoch 2 Batch 350 Loss 2.385073184967041\n",
      "Epoch 2 Batch 360 Loss 2.3303868770599365\n",
      "Epoch 2 Batch 370 Loss 2.7383780479431152\n",
      "Epoch 2 Batch 380 Loss 2.611438035964966\n",
      "Epoch 2 Batch 390 Loss 2.55561900138855\n",
      "Epoch 2 Batch 400 Loss 2.3309686183929443\n",
      "Epoch 2 Batch 410 Loss 2.7827072143554688\n",
      "Epoch 2 Batch 420 Loss 2.3651340007781982\n",
      "Epoch 2 Batch 430 Loss 2.740443706512451\n",
      "Epoch 2 Batch 440 Loss 2.6355817317962646\n",
      "Epoch 2 Batch 450 Loss 2.5375750064849854\n",
      "Epoch 2 Batch 460 Loss 2.734774351119995\n",
      "Epoch 2 Batch 470 Loss 2.789684295654297\n",
      "Epoch 2 Batch 480 Loss 2.4283218383789062\n",
      "Epoch 2 Batch 490 Loss 2.6398556232452393\n",
      "Epoch 2 Batch 500 Loss 2.4662704467773438\n",
      "Epoch 2 Batch 510 Loss 2.4435064792633057\n",
      "Epoch 2 Batch 520 Loss 2.2980058193206787\n",
      "Epoch 2 Batch 530 Loss 2.6761276721954346\n",
      "Epoch 2 Batch 540 Loss 2.3089101314544678\n",
      "Epoch 2 Batch 550 Loss 2.4668216705322266\n",
      "Epoch 2 Batch 560 Loss 2.5437066555023193\n",
      "Epoch 2 Batch 570 Loss 2.524726629257202\n",
      "Epoch 2 Batch 580 Loss 2.76170015335083\n",
      "Epoch 2 Batch 590 Loss 2.413618326187134\n",
      "Epoch 2 Batch 600 Loss 2.483243703842163\n",
      "Epoch 2 Batch 610 Loss 2.463808536529541\n",
      "Epoch 2 Batch 620 Loss 2.6865458488464355\n",
      "Epoch 2 Batch 630 Loss 2.227785348892212\n",
      "Epoch 2 Batch 640 Loss 2.6080245971679688\n",
      "Epoch 2 Batch 650 Loss 2.5004992485046387\n",
      "Epoch 2 Batch 660 Loss 2.2822256088256836\n",
      "Epoch 2 Batch 670 Loss 2.5552544593811035\n",
      "Epoch 2 Batch 680 Loss 2.428191900253296\n",
      "Epoch 2 Batch 690 Loss 2.3056583404541016\n",
      "Epoch 2 Batch 700 Loss 2.360696315765381\n",
      "Epoch 2 Batch 710 Loss 2.434138059616089\n",
      "Epoch 2 Batch 720 Loss 2.4443252086639404\n",
      "Epoch 2 Batch 730 Loss 2.18078875541687\n",
      "Epoch 2 Batch 740 Loss 2.5196683406829834\n",
      "Epoch 2 Batch 750 Loss 2.3928205966949463\n",
      "Epoch 2 Batch 760 Loss 2.3124730587005615\n",
      "Epoch 2 Batch 770 Loss 2.5475499629974365\n",
      "Epoch 2 Batch 780 Loss 2.6244893074035645\n",
      "Epoch 2 Batch 790 Loss 2.581475019454956\n",
      "Epoch 2 Batch 800 Loss 2.324410915374756\n",
      "Epoch 2 Batch 810 Loss 2.4287209510803223\n",
      "Epoch 2 Batch 820 Loss 2.311021089553833\n",
      "Epoch 2 Batch 830 Loss 2.2947070598602295\n",
      "Epoch 2 Batch 840 Loss 2.65505313873291\n",
      "Epoch 2 Batch 850 Loss 2.693833589553833\n",
      "Epoch 2 Batch 860 Loss 2.5885307788848877\n",
      "Epoch 2 Batch 870 Loss 2.3239829540252686\n",
      "Epoch 2 Batch 880 Loss 2.4388153553009033\n",
      "Epoch 2 Batch 890 Loss 2.5719616413116455\n",
      "Epoch 2 Batch 900 Loss 2.4676601886749268\n",
      "Epoch 2 Batch 910 Loss 2.352870225906372\n",
      "Epoch 2 Batch 920 Loss 2.677096128463745\n",
      "Epoch 2 Batch 930 Loss 2.6111245155334473\n",
      "Epoch 2 Batch 940 Loss 2.573077440261841\n",
      "Epoch 2 Batch 950 Loss 2.4819602966308594\n",
      "Epoch 2 Batch 960 Loss 2.5384953022003174\n",
      "Epoch 2 Batch 970 Loss 2.6141884326934814\n",
      "Epoch 2 Batch 980 Loss 2.2761101722717285\n",
      "Epoch 2 Batch 990 Loss 2.5974526405334473\n",
      "Epoch 2 Batch 1000 Loss 2.517937660217285\n",
      "Epoch 2 Batch 1010 Loss 2.3936548233032227\n",
      "Epoch 2 Batch 1020 Loss 2.7090671062469482\n",
      "Epoch 2 Batch 1030 Loss 2.5341169834136963\n",
      "Epoch 2 Loss 2.5230\n",
      "Epoch 3 Batch 0 Loss 2.5759732723236084\n",
      "Epoch 3 Batch 10 Loss 2.437462568283081\n",
      "Epoch 3 Batch 20 Loss 2.414961576461792\n",
      "Epoch 3 Batch 30 Loss 2.760024309158325\n",
      "Epoch 3 Batch 40 Loss 2.4454712867736816\n",
      "Epoch 3 Batch 50 Loss 2.5728847980499268\n",
      "Epoch 3 Batch 60 Loss 2.424903631210327\n",
      "Epoch 3 Batch 70 Loss 2.491802930831909\n",
      "Epoch 3 Batch 80 Loss 2.439239263534546\n",
      "Epoch 3 Batch 90 Loss 2.324336290359497\n",
      "Epoch 3 Batch 100 Loss 2.6920571327209473\n",
      "Epoch 3 Batch 110 Loss 2.4773573875427246\n",
      "Epoch 3 Batch 120 Loss 2.6692299842834473\n",
      "Epoch 3 Batch 130 Loss 2.1504645347595215\n",
      "Epoch 3 Batch 140 Loss 2.4621546268463135\n",
      "Epoch 3 Batch 150 Loss 2.4813296794891357\n",
      "Epoch 3 Batch 160 Loss 2.6069881916046143\n",
      "Epoch 3 Batch 170 Loss 2.440218687057495\n",
      "Epoch 3 Batch 180 Loss 2.505234479904175\n",
      "Epoch 3 Batch 190 Loss 2.4216346740722656\n",
      "Epoch 3 Batch 200 Loss 2.334693670272827\n",
      "Epoch 3 Batch 210 Loss 2.3561112880706787\n",
      "Epoch 3 Batch 220 Loss 2.4217281341552734\n",
      "Epoch 3 Batch 230 Loss 2.488429546356201\n",
      "Epoch 3 Batch 240 Loss 2.180417776107788\n",
      "Epoch 3 Batch 250 Loss 2.1393275260925293\n",
      "Epoch 3 Batch 260 Loss 2.286226749420166\n",
      "Epoch 3 Batch 270 Loss 2.425607204437256\n",
      "Epoch 3 Batch 280 Loss 2.497363805770874\n",
      "Epoch 3 Batch 290 Loss 2.3199496269226074\n",
      "Epoch 3 Batch 300 Loss 2.508274793624878\n",
      "Epoch 3 Batch 310 Loss 2.246997594833374\n",
      "Epoch 3 Batch 320 Loss 2.2244644165039062\n",
      "Epoch 3 Batch 330 Loss 2.502415418624878\n",
      "Epoch 3 Batch 340 Loss 2.387375593185425\n",
      "Epoch 3 Batch 350 Loss 2.2172393798828125\n",
      "Epoch 3 Batch 360 Loss 2.391329526901245\n",
      "Epoch 3 Batch 370 Loss 2.5181877613067627\n",
      "Epoch 3 Batch 380 Loss 2.560539722442627\n",
      "Epoch 3 Batch 390 Loss 2.153189182281494\n",
      "Epoch 3 Batch 400 Loss 2.6388895511627197\n",
      "Epoch 3 Batch 410 Loss 2.216989517211914\n",
      "Epoch 3 Batch 420 Loss 2.3714020252227783\n",
      "Epoch 3 Batch 430 Loss 2.578049898147583\n",
      "Epoch 3 Batch 440 Loss 2.494645357131958\n",
      "Epoch 3 Batch 450 Loss 2.1486763954162598\n",
      "Epoch 3 Batch 460 Loss 2.588261842727661\n",
      "Epoch 3 Batch 470 Loss 2.305196523666382\n",
      "Epoch 3 Batch 480 Loss 2.360577344894409\n",
      "Epoch 3 Batch 490 Loss 2.6287262439727783\n",
      "Epoch 3 Batch 500 Loss 2.565368413925171\n",
      "Epoch 3 Batch 510 Loss 2.3001296520233154\n",
      "Epoch 3 Batch 520 Loss 2.502603530883789\n",
      "Epoch 3 Batch 530 Loss 2.4905152320861816\n",
      "Epoch 3 Batch 540 Loss 2.477346420288086\n",
      "Epoch 3 Batch 550 Loss 2.4294285774230957\n",
      "Epoch 3 Batch 560 Loss 2.4800260066986084\n",
      "Epoch 3 Batch 570 Loss 2.2808308601379395\n",
      "Epoch 3 Batch 580 Loss 2.341442823410034\n",
      "Epoch 3 Batch 590 Loss 2.5841386318206787\n",
      "Epoch 3 Batch 600 Loss 2.5652596950531006\n",
      "Epoch 3 Batch 610 Loss 2.6812779903411865\n",
      "Epoch 3 Batch 620 Loss 2.5533649921417236\n",
      "Epoch 3 Batch 630 Loss 2.514378309249878\n",
      "Epoch 3 Batch 640 Loss 2.6064350605010986\n",
      "Epoch 3 Batch 650 Loss 2.517857313156128\n",
      "Epoch 3 Batch 660 Loss 2.2235805988311768\n",
      "Epoch 3 Batch 670 Loss 2.3442680835723877\n",
      "Epoch 3 Batch 680 Loss 2.2628254890441895\n",
      "Epoch 3 Batch 690 Loss 2.3747401237487793\n",
      "Epoch 3 Batch 700 Loss 2.5611050128936768\n",
      "Epoch 3 Batch 710 Loss 2.2297017574310303\n",
      "Epoch 3 Batch 720 Loss 2.409264087677002\n",
      "Epoch 3 Batch 730 Loss 2.3513410091400146\n",
      "Epoch 3 Batch 740 Loss 2.3050663471221924\n",
      "Epoch 3 Batch 750 Loss 2.607591390609741\n",
      "Epoch 3 Batch 760 Loss 1.9995321035385132\n",
      "Epoch 3 Batch 770 Loss 2.263451099395752\n",
      "Epoch 3 Batch 780 Loss 2.218280553817749\n",
      "Epoch 3 Batch 790 Loss 2.345216989517212\n",
      "Epoch 3 Batch 800 Loss 2.4865612983703613\n",
      "Epoch 3 Batch 810 Loss 2.386840343475342\n",
      "Epoch 3 Batch 820 Loss 2.4154245853424072\n",
      "Epoch 3 Batch 830 Loss 2.3125858306884766\n",
      "Epoch 3 Batch 840 Loss 2.4409520626068115\n",
      "Epoch 3 Batch 850 Loss 2.418412208557129\n",
      "Epoch 3 Batch 860 Loss 2.26690673828125\n",
      "Epoch 3 Batch 870 Loss 2.271601915359497\n",
      "Epoch 3 Batch 880 Loss 2.1821305751800537\n",
      "Epoch 3 Batch 890 Loss 2.4185702800750732\n",
      "Epoch 3 Batch 900 Loss 2.2936646938323975\n",
      "Epoch 3 Batch 910 Loss 2.1966652870178223\n",
      "Epoch 3 Batch 920 Loss 2.4455955028533936\n",
      "Epoch 3 Batch 930 Loss 2.334831476211548\n",
      "Epoch 3 Batch 940 Loss 2.260772466659546\n",
      "Epoch 3 Batch 950 Loss 2.528507709503174\n",
      "Epoch 3 Batch 960 Loss 2.4644827842712402\n",
      "Epoch 3 Batch 970 Loss 2.3264942169189453\n",
      "Epoch 3 Batch 980 Loss 2.1960206031799316\n",
      "Epoch 3 Batch 990 Loss 2.2948875427246094\n",
      "Epoch 3 Batch 1000 Loss 2.3406827449798584\n",
      "Epoch 3 Batch 1010 Loss 2.2357423305511475\n",
      "Epoch 3 Batch 1020 Loss 2.3105392456054688\n",
      "Epoch 3 Batch 1030 Loss 2.406827688217163\n",
      "Epoch 3 Loss 2.4026\n",
      "Epoch 4 Batch 0 Loss 2.217430830001831\n",
      "Epoch 4 Batch 10 Loss 2.4384419918060303\n",
      "Epoch 4 Batch 20 Loss 2.375939130783081\n",
      "Epoch 4 Batch 30 Loss 2.2891201972961426\n",
      "Epoch 4 Batch 40 Loss 2.3091697692871094\n",
      "Epoch 4 Batch 50 Loss 2.37311053276062\n",
      "Epoch 4 Batch 60 Loss 2.248206615447998\n",
      "Epoch 4 Batch 70 Loss 2.345914125442505\n",
      "Epoch 4 Batch 80 Loss 2.3662784099578857\n",
      "Epoch 4 Batch 90 Loss 2.2149250507354736\n",
      "Epoch 4 Batch 100 Loss 2.372636556625366\n",
      "Epoch 4 Batch 110 Loss 2.164124011993408\n",
      "Epoch 4 Batch 120 Loss 2.098271131515503\n",
      "Epoch 4 Batch 130 Loss 2.3536789417266846\n",
      "Epoch 4 Batch 140 Loss 2.222407341003418\n",
      "Epoch 4 Batch 150 Loss 2.081486463546753\n",
      "Epoch 4 Batch 160 Loss 2.1703929901123047\n",
      "Epoch 4 Batch 170 Loss 2.324079990386963\n",
      "Epoch 4 Batch 180 Loss 2.5283405780792236\n",
      "Epoch 4 Batch 190 Loss 2.203510284423828\n",
      "Epoch 4 Batch 200 Loss 2.3612310886383057\n",
      "Epoch 4 Batch 210 Loss 2.189906120300293\n",
      "Epoch 4 Batch 220 Loss 2.3579070568084717\n",
      "Epoch 4 Batch 230 Loss 2.597378969192505\n",
      "Epoch 4 Batch 240 Loss 2.364088773727417\n",
      "Epoch 4 Batch 250 Loss 1.9599155187606812\n",
      "Epoch 4 Batch 260 Loss 2.0237152576446533\n",
      "Epoch 4 Batch 270 Loss 2.300741195678711\n",
      "Epoch 4 Batch 280 Loss 2.2464282512664795\n",
      "Epoch 4 Batch 290 Loss 2.203608751296997\n",
      "Epoch 4 Batch 300 Loss 2.293020009994507\n",
      "Epoch 4 Batch 310 Loss 2.4160048961639404\n",
      "Epoch 4 Batch 320 Loss 2.1401543617248535\n",
      "Epoch 4 Batch 330 Loss 2.213914632797241\n",
      "Epoch 4 Batch 340 Loss 2.395005941390991\n",
      "Epoch 4 Batch 350 Loss 2.633324146270752\n",
      "Epoch 4 Batch 360 Loss 2.421445846557617\n",
      "Epoch 4 Batch 370 Loss 2.264177083969116\n",
      "Epoch 4 Batch 380 Loss 2.4429214000701904\n",
      "Epoch 4 Batch 390 Loss 2.312828540802002\n",
      "Epoch 4 Batch 400 Loss 2.304302215576172\n",
      "Epoch 4 Batch 410 Loss 2.445085287094116\n",
      "Epoch 4 Batch 420 Loss 1.974012017250061\n",
      "Epoch 4 Batch 430 Loss 2.1480484008789062\n",
      "Epoch 4 Batch 440 Loss 2.336350679397583\n",
      "Epoch 4 Batch 450 Loss 2.474961519241333\n",
      "Epoch 4 Batch 460 Loss 2.279165029525757\n",
      "Epoch 4 Batch 470 Loss 2.0943305492401123\n",
      "Epoch 4 Batch 480 Loss 2.253955364227295\n",
      "Epoch 4 Batch 490 Loss 2.2335522174835205\n",
      "Epoch 4 Batch 500 Loss 2.2614057064056396\n",
      "Epoch 4 Batch 510 Loss 2.0161550045013428\n",
      "Epoch 4 Batch 520 Loss 2.0731735229492188\n",
      "Epoch 4 Batch 530 Loss 2.1717517375946045\n",
      "Epoch 4 Batch 540 Loss 2.474736452102661\n",
      "Epoch 4 Batch 550 Loss 2.218019485473633\n",
      "Epoch 4 Batch 560 Loss 2.0757558345794678\n",
      "Epoch 4 Batch 570 Loss 2.012064218521118\n",
      "Epoch 4 Batch 580 Loss 2.310049295425415\n",
      "Epoch 4 Batch 590 Loss 2.116326093673706\n",
      "Epoch 4 Batch 600 Loss 2.4337005615234375\n",
      "Epoch 4 Batch 610 Loss 2.3255958557128906\n",
      "Epoch 4 Batch 620 Loss 2.46174693107605\n",
      "Epoch 4 Batch 630 Loss 2.1366138458251953\n",
      "Epoch 4 Batch 640 Loss 2.297011137008667\n",
      "Epoch 4 Batch 650 Loss 2.5406436920166016\n",
      "Epoch 4 Batch 660 Loss 2.2721309661865234\n",
      "Epoch 4 Batch 670 Loss 2.2786858081817627\n",
      "Epoch 4 Batch 680 Loss 2.657970428466797\n",
      "Epoch 4 Batch 690 Loss 2.1926827430725098\n",
      "Epoch 4 Batch 700 Loss 2.5194194316864014\n",
      "Epoch 4 Batch 710 Loss 2.384382724761963\n",
      "Epoch 4 Batch 720 Loss 2.3767898082733154\n",
      "Epoch 4 Batch 730 Loss 2.2518720626831055\n",
      "Epoch 4 Batch 740 Loss 2.443694829940796\n",
      "Epoch 4 Batch 750 Loss 1.9704784154891968\n",
      "Epoch 4 Batch 760 Loss 2.421900510787964\n",
      "Epoch 4 Batch 770 Loss 2.300996780395508\n",
      "Epoch 4 Batch 780 Loss 2.614718198776245\n",
      "Epoch 4 Batch 790 Loss 2.369206190109253\n",
      "Epoch 4 Batch 800 Loss 2.316974639892578\n",
      "Epoch 4 Batch 810 Loss 2.0980923175811768\n",
      "Epoch 4 Batch 820 Loss 2.1804864406585693\n",
      "Epoch 4 Batch 830 Loss 2.4449880123138428\n",
      "Epoch 4 Batch 840 Loss 2.2678699493408203\n",
      "Epoch 4 Batch 850 Loss 2.371736526489258\n",
      "Epoch 4 Batch 860 Loss 2.507075071334839\n",
      "Epoch 4 Batch 870 Loss 2.218247890472412\n",
      "Epoch 4 Batch 880 Loss 2.281249761581421\n",
      "Epoch 4 Batch 890 Loss 2.3573687076568604\n",
      "Epoch 4 Batch 900 Loss 2.3131699562072754\n",
      "Epoch 4 Batch 910 Loss 2.229391574859619\n",
      "Epoch 4 Batch 920 Loss 2.13118052482605\n",
      "Epoch 4 Batch 930 Loss 2.3328635692596436\n",
      "Epoch 4 Batch 940 Loss 2.2452392578125\n",
      "Epoch 4 Batch 950 Loss 2.1415021419525146\n",
      "Epoch 4 Batch 960 Loss 2.3280491828918457\n",
      "Epoch 4 Batch 970 Loss 1.9927705526351929\n",
      "Epoch 4 Batch 980 Loss 2.172283172607422\n",
      "Epoch 4 Batch 990 Loss 2.283027410507202\n",
      "Epoch 4 Batch 1000 Loss 2.3551433086395264\n",
      "Epoch 4 Batch 1010 Loss 2.2064526081085205\n",
      "Epoch 4 Batch 1020 Loss 2.1087234020233154\n",
      "Epoch 4 Batch 1030 Loss 2.2370553016662598\n",
      "Epoch 4 Loss 2.3010\n",
      "Epoch 5 Batch 0 Loss 2.157780408859253\n",
      "Epoch 5 Batch 10 Loss 2.2915618419647217\n",
      "Epoch 5 Batch 20 Loss 2.395963668823242\n",
      "Epoch 5 Batch 30 Loss 2.1844491958618164\n",
      "Epoch 5 Batch 40 Loss 2.2637031078338623\n",
      "Epoch 5 Batch 50 Loss 2.311650037765503\n",
      "Epoch 5 Batch 60 Loss 2.242932081222534\n",
      "Epoch 5 Batch 70 Loss 2.2479941844940186\n",
      "Epoch 5 Batch 80 Loss 2.0779945850372314\n",
      "Epoch 5 Batch 90 Loss 2.1153697967529297\n",
      "Epoch 5 Batch 100 Loss 2.285856008529663\n",
      "Epoch 5 Batch 110 Loss 2.252427101135254\n",
      "Epoch 5 Batch 120 Loss 2.2171895503997803\n",
      "Epoch 5 Batch 130 Loss 2.1401398181915283\n",
      "Epoch 5 Batch 140 Loss 2.1033036708831787\n",
      "Epoch 5 Batch 150 Loss 2.2079546451568604\n",
      "Epoch 5 Batch 160 Loss 2.455801248550415\n",
      "Epoch 5 Batch 170 Loss 1.973173975944519\n",
      "Epoch 5 Batch 180 Loss 2.246948003768921\n",
      "Epoch 5 Batch 190 Loss 2.561779499053955\n",
      "Epoch 5 Batch 200 Loss 1.9907411336898804\n",
      "Epoch 5 Batch 210 Loss 2.5044896602630615\n",
      "Epoch 5 Batch 220 Loss 2.0703630447387695\n",
      "Epoch 5 Batch 230 Loss 2.1151483058929443\n",
      "Epoch 5 Batch 240 Loss 2.333902359008789\n",
      "Epoch 5 Batch 250 Loss 2.049976348876953\n",
      "Epoch 5 Batch 260 Loss 2.262037992477417\n",
      "Epoch 5 Batch 270 Loss 2.295053720474243\n",
      "Epoch 5 Batch 280 Loss 2.1895534992218018\n",
      "Epoch 5 Batch 290 Loss 2.1346404552459717\n",
      "Epoch 5 Batch 300 Loss 2.1720426082611084\n",
      "Epoch 5 Batch 310 Loss 2.3560519218444824\n",
      "Epoch 5 Batch 320 Loss 2.410423994064331\n",
      "Epoch 5 Batch 330 Loss 2.030719518661499\n",
      "Epoch 5 Batch 340 Loss 2.237028121948242\n",
      "Epoch 5 Batch 350 Loss 2.4383299350738525\n",
      "Epoch 5 Batch 360 Loss 2.439417600631714\n",
      "Epoch 5 Batch 370 Loss 2.3853917121887207\n",
      "Epoch 5 Batch 380 Loss 1.865674614906311\n",
      "Epoch 5 Batch 390 Loss 2.1756176948547363\n",
      "Epoch 5 Batch 400 Loss 2.129506826400757\n",
      "Epoch 5 Batch 410 Loss 1.979042887687683\n",
      "Epoch 5 Batch 420 Loss 2.294309377670288\n",
      "Epoch 5 Batch 430 Loss 2.136995315551758\n",
      "Epoch 5 Batch 440 Loss 2.1885900497436523\n",
      "Epoch 5 Batch 450 Loss 2.2741641998291016\n",
      "Epoch 5 Batch 460 Loss 2.18066668510437\n",
      "Epoch 5 Batch 470 Loss 2.073068380355835\n",
      "Epoch 5 Batch 480 Loss 2.3625106811523438\n",
      "Epoch 5 Batch 490 Loss 2.3518013954162598\n",
      "Epoch 5 Batch 500 Loss 2.2828927040100098\n",
      "Epoch 5 Batch 510 Loss 2.115154266357422\n",
      "Epoch 5 Batch 520 Loss 2.1847572326660156\n",
      "Epoch 5 Batch 530 Loss 2.604997158050537\n",
      "Epoch 5 Batch 540 Loss 2.3622472286224365\n",
      "Epoch 5 Batch 550 Loss 2.0451440811157227\n",
      "Epoch 5 Batch 560 Loss 2.106314182281494\n",
      "Epoch 5 Batch 570 Loss 2.2213428020477295\n",
      "Epoch 5 Batch 580 Loss 2.2834203243255615\n",
      "Epoch 5 Batch 590 Loss 1.969054102897644\n",
      "Epoch 5 Batch 600 Loss 2.315432071685791\n",
      "Epoch 5 Batch 610 Loss 2.276296615600586\n",
      "Epoch 5 Batch 620 Loss 2.206944227218628\n",
      "Epoch 5 Batch 630 Loss 2.3358845710754395\n",
      "Epoch 5 Batch 640 Loss 2.244688034057617\n",
      "Epoch 5 Batch 650 Loss 1.9475936889648438\n",
      "Epoch 5 Batch 660 Loss 2.2813103199005127\n",
      "Epoch 5 Batch 670 Loss 2.0187337398529053\n",
      "Epoch 5 Batch 680 Loss 2.389160394668579\n",
      "Epoch 5 Batch 690 Loss 2.073852300643921\n",
      "Epoch 5 Batch 700 Loss 2.1784825325012207\n",
      "Epoch 5 Batch 710 Loss 2.123095750808716\n",
      "Epoch 5 Batch 720 Loss 2.3105199337005615\n",
      "Epoch 5 Batch 730 Loss 2.3383421897888184\n",
      "Epoch 5 Batch 740 Loss 1.9486576318740845\n",
      "Epoch 5 Batch 750 Loss 2.2984745502471924\n",
      "Epoch 5 Batch 760 Loss 2.179518222808838\n",
      "Epoch 5 Batch 770 Loss 2.1079654693603516\n",
      "Epoch 5 Batch 780 Loss 2.263463258743286\n",
      "Epoch 5 Batch 790 Loss 2.3098843097686768\n",
      "Epoch 5 Batch 800 Loss 2.067349910736084\n",
      "Epoch 5 Batch 810 Loss 2.108928918838501\n",
      "Epoch 5 Batch 820 Loss 2.233008623123169\n",
      "Epoch 5 Batch 830 Loss 2.1207222938537598\n",
      "Epoch 5 Batch 840 Loss 2.2830631732940674\n",
      "Epoch 5 Batch 850 Loss 2.0807957649230957\n",
      "Epoch 5 Batch 860 Loss 2.2336995601654053\n",
      "Epoch 5 Batch 870 Loss 2.1910431385040283\n",
      "Epoch 5 Batch 880 Loss 1.9802874326705933\n",
      "Epoch 5 Batch 890 Loss 2.1587412357330322\n",
      "Epoch 5 Batch 900 Loss 2.2667343616485596\n",
      "Epoch 5 Batch 910 Loss 1.911755919456482\n",
      "Epoch 5 Batch 920 Loss 2.0491483211517334\n",
      "Epoch 5 Batch 930 Loss 2.125845432281494\n",
      "Epoch 5 Batch 940 Loss 2.321795701980591\n",
      "Epoch 5 Batch 950 Loss 2.2600009441375732\n",
      "Epoch 5 Batch 960 Loss 2.1523854732513428\n",
      "Epoch 5 Batch 970 Loss 2.4289486408233643\n",
      "Epoch 5 Batch 980 Loss 2.462355375289917\n",
      "Epoch 5 Batch 990 Loss 2.121215581893921\n",
      "Epoch 5 Batch 1000 Loss 2.237575054168701\n",
      "Epoch 5 Batch 1010 Loss 2.1436874866485596\n",
      "Epoch 5 Batch 1020 Loss 2.2548491954803467\n",
      "Epoch 5 Batch 1030 Loss 2.3021438121795654\n",
      "Epoch 5 Loss 2.2208\n",
      "Epoch 6 Batch 0 Loss 2.0728752613067627\n",
      "Epoch 6 Batch 10 Loss 2.173818588256836\n",
      "Epoch 6 Batch 20 Loss 2.2235238552093506\n",
      "Epoch 6 Batch 30 Loss 2.0980587005615234\n",
      "Epoch 6 Batch 40 Loss 1.8648195266723633\n",
      "Epoch 6 Batch 50 Loss 2.207829236984253\n",
      "Epoch 6 Batch 60 Loss 2.1056056022644043\n",
      "Epoch 6 Batch 70 Loss 1.9781687259674072\n",
      "Epoch 6 Batch 80 Loss 2.305128812789917\n",
      "Epoch 6 Batch 90 Loss 2.039944648742676\n",
      "Epoch 6 Batch 100 Loss 1.9726094007492065\n",
      "Epoch 6 Batch 110 Loss 2.2531239986419678\n",
      "Epoch 6 Batch 120 Loss 2.12400484085083\n",
      "Epoch 6 Batch 130 Loss 2.035468578338623\n",
      "Epoch 6 Batch 140 Loss 1.9746731519699097\n",
      "Epoch 6 Batch 150 Loss 2.3513505458831787\n",
      "Epoch 6 Batch 160 Loss 2.2407310009002686\n",
      "Epoch 6 Batch 170 Loss 2.142184257507324\n",
      "Epoch 6 Batch 180 Loss 2.0859897136688232\n",
      "Epoch 6 Batch 190 Loss 2.214534282684326\n",
      "Epoch 6 Batch 200 Loss 2.0811681747436523\n",
      "Epoch 6 Batch 210 Loss 1.9690618515014648\n",
      "Epoch 6 Batch 220 Loss 1.9742987155914307\n",
      "Epoch 6 Batch 230 Loss 2.1457412242889404\n",
      "Epoch 6 Batch 240 Loss 2.0713865756988525\n",
      "Epoch 6 Batch 250 Loss 2.0796329975128174\n",
      "Epoch 6 Batch 260 Loss 2.1655476093292236\n",
      "Epoch 6 Batch 270 Loss 2.012047290802002\n",
      "Epoch 6 Batch 280 Loss 2.138237476348877\n",
      "Epoch 6 Batch 290 Loss 2.161472797393799\n",
      "Epoch 6 Batch 300 Loss 2.035815954208374\n",
      "Epoch 6 Batch 310 Loss 2.0321261882781982\n",
      "Epoch 6 Batch 320 Loss 1.9769887924194336\n",
      "Epoch 6 Batch 330 Loss 2.2251758575439453\n",
      "Epoch 6 Batch 340 Loss 2.1465039253234863\n",
      "Epoch 6 Batch 350 Loss 2.132591962814331\n",
      "Epoch 6 Batch 360 Loss 1.8693796396255493\n",
      "Epoch 6 Batch 370 Loss 2.2155120372772217\n",
      "Epoch 6 Batch 380 Loss 2.159479856491089\n",
      "Epoch 6 Batch 390 Loss 2.200209379196167\n",
      "Epoch 6 Batch 400 Loss 2.1541500091552734\n",
      "Epoch 6 Batch 410 Loss 2.20597767829895\n",
      "Epoch 6 Batch 420 Loss 2.3133811950683594\n",
      "Epoch 6 Batch 430 Loss 1.9544578790664673\n",
      "Epoch 6 Batch 440 Loss 2.2309415340423584\n",
      "Epoch 6 Batch 450 Loss 2.163100004196167\n",
      "Epoch 6 Batch 460 Loss 2.2525429725646973\n",
      "Epoch 6 Batch 470 Loss 2.1876747608184814\n",
      "Epoch 6 Batch 480 Loss 1.961064338684082\n",
      "Epoch 6 Batch 490 Loss 2.0507237911224365\n",
      "Epoch 6 Batch 500 Loss 2.1733694076538086\n",
      "Epoch 6 Batch 510 Loss 2.081246852874756\n",
      "Epoch 6 Batch 520 Loss 2.2333984375\n",
      "Epoch 6 Batch 530 Loss 2.292753219604492\n",
      "Epoch 6 Batch 540 Loss 2.0036189556121826\n",
      "Epoch 6 Batch 550 Loss 2.2176339626312256\n",
      "Epoch 6 Batch 560 Loss 2.203572988510132\n",
      "Epoch 6 Batch 570 Loss 2.3071725368499756\n",
      "Epoch 6 Batch 580 Loss 2.262449264526367\n",
      "Epoch 6 Batch 590 Loss 2.08463716506958\n",
      "Epoch 6 Batch 600 Loss 2.296462297439575\n",
      "Epoch 6 Batch 610 Loss 2.244947910308838\n",
      "Epoch 6 Batch 620 Loss 2.0913636684417725\n",
      "Epoch 6 Batch 630 Loss 2.1829335689544678\n",
      "Epoch 6 Batch 640 Loss 2.2457234859466553\n",
      "Epoch 6 Batch 650 Loss 2.066662549972534\n",
      "Epoch 6 Batch 660 Loss 2.180856704711914\n",
      "Epoch 6 Batch 670 Loss 2.0863442420959473\n",
      "Epoch 6 Batch 680 Loss 2.241823673248291\n",
      "Epoch 6 Batch 690 Loss 1.9474594593048096\n",
      "Epoch 6 Batch 700 Loss 1.9734169244766235\n",
      "Epoch 6 Batch 710 Loss 2.13797926902771\n",
      "Epoch 6 Batch 720 Loss 2.1039180755615234\n",
      "Epoch 6 Batch 730 Loss 2.0881969928741455\n",
      "Epoch 6 Batch 740 Loss 2.236091375350952\n",
      "Epoch 6 Batch 750 Loss 2.1045374870300293\n",
      "Epoch 6 Batch 760 Loss 1.990581750869751\n",
      "Epoch 6 Batch 770 Loss 2.065372943878174\n",
      "Epoch 6 Batch 780 Loss 2.2412376403808594\n",
      "Epoch 6 Batch 790 Loss 2.0077810287475586\n",
      "Epoch 6 Batch 800 Loss 2.0344855785369873\n",
      "Epoch 6 Batch 810 Loss 1.973262071609497\n",
      "Epoch 6 Batch 820 Loss 2.06876540184021\n",
      "Epoch 6 Batch 830 Loss 1.8417431116104126\n",
      "Epoch 6 Batch 840 Loss 2.2797372341156006\n",
      "Epoch 6 Batch 850 Loss 2.2023487091064453\n",
      "Epoch 6 Batch 860 Loss 2.0832841396331787\n",
      "Epoch 6 Batch 870 Loss 2.0488221645355225\n",
      "Epoch 6 Batch 880 Loss 2.1011366844177246\n",
      "Epoch 6 Batch 890 Loss 2.3173654079437256\n",
      "Epoch 6 Batch 900 Loss 1.9303356409072876\n",
      "Epoch 6 Batch 910 Loss 2.065999746322632\n",
      "Epoch 6 Batch 920 Loss 1.9848765134811401\n",
      "Epoch 6 Batch 930 Loss 2.031144857406616\n",
      "Epoch 6 Batch 940 Loss 1.815410852432251\n",
      "Epoch 6 Batch 950 Loss 2.2866344451904297\n",
      "Epoch 6 Batch 960 Loss 1.9840935468673706\n",
      "Epoch 6 Batch 970 Loss 2.067430257797241\n",
      "Epoch 6 Batch 980 Loss 2.142699718475342\n",
      "Epoch 6 Batch 990 Loss 2.1400697231292725\n",
      "Epoch 6 Batch 1000 Loss 2.2181894779205322\n",
      "Epoch 6 Batch 1010 Loss 2.244097948074341\n",
      "Epoch 6 Batch 1020 Loss 2.001878023147583\n",
      "Epoch 6 Batch 1030 Loss 2.081472873687744\n",
      "Epoch 6 Loss 2.1429\n",
      "Epoch 7 Batch 0 Loss 2.10243821144104\n",
      "Epoch 7 Batch 10 Loss 2.017207384109497\n",
      "Epoch 7 Batch 20 Loss 2.143866539001465\n",
      "Epoch 7 Batch 30 Loss 1.906083106994629\n",
      "Epoch 7 Batch 40 Loss 2.0915279388427734\n",
      "Epoch 7 Batch 50 Loss 2.223543882369995\n",
      "Epoch 7 Batch 60 Loss 1.8202780485153198\n",
      "Epoch 7 Batch 70 Loss 1.913926362991333\n",
      "Epoch 7 Batch 80 Loss 2.2011735439300537\n",
      "Epoch 7 Batch 90 Loss 2.0320706367492676\n",
      "Epoch 7 Batch 100 Loss 2.05908203125\n",
      "Epoch 7 Batch 110 Loss 2.0337209701538086\n",
      "Epoch 7 Batch 120 Loss 1.9261549711227417\n",
      "Epoch 7 Batch 130 Loss 1.9442652463912964\n",
      "Epoch 7 Batch 140 Loss 1.992104172706604\n",
      "Epoch 7 Batch 150 Loss 2.0905375480651855\n",
      "Epoch 7 Batch 160 Loss 1.9850319623947144\n",
      "Epoch 7 Batch 170 Loss 2.180593967437744\n",
      "Epoch 7 Batch 180 Loss 2.236881732940674\n",
      "Epoch 7 Batch 190 Loss 2.1723790168762207\n",
      "Epoch 7 Batch 200 Loss 2.138134717941284\n",
      "Epoch 7 Batch 210 Loss 1.91594660282135\n",
      "Epoch 7 Batch 220 Loss 2.0076940059661865\n",
      "Epoch 7 Batch 230 Loss 1.9603004455566406\n",
      "Epoch 7 Batch 240 Loss 2.134873151779175\n",
      "Epoch 7 Batch 250 Loss 2.0129830837249756\n",
      "Epoch 7 Batch 260 Loss 2.238255262374878\n",
      "Epoch 7 Batch 270 Loss 1.844787836074829\n",
      "Epoch 7 Batch 280 Loss 2.062575101852417\n",
      "Epoch 7 Batch 290 Loss 2.2499425411224365\n",
      "Epoch 7 Batch 300 Loss 2.306370496749878\n",
      "Epoch 7 Batch 310 Loss 2.2657418251037598\n",
      "Epoch 7 Batch 320 Loss 2.163374662399292\n",
      "Epoch 7 Batch 330 Loss 2.269225597381592\n",
      "Epoch 7 Batch 340 Loss 1.9321262836456299\n",
      "Epoch 7 Batch 350 Loss 2.1490745544433594\n",
      "Epoch 7 Batch 360 Loss 2.4300296306610107\n",
      "Epoch 7 Batch 370 Loss 1.9315805435180664\n",
      "Epoch 7 Batch 380 Loss 1.901224136352539\n",
      "Epoch 7 Batch 390 Loss 1.936259150505066\n",
      "Epoch 7 Batch 400 Loss 2.16162109375\n",
      "Epoch 7 Batch 410 Loss 2.088428497314453\n",
      "Epoch 7 Batch 420 Loss 1.9973171949386597\n",
      "Epoch 7 Batch 430 Loss 2.024787187576294\n",
      "Epoch 7 Batch 440 Loss 2.0865848064422607\n",
      "Epoch 7 Batch 450 Loss 1.9746192693710327\n",
      "Epoch 7 Batch 460 Loss 2.171272039413452\n",
      "Epoch 7 Batch 470 Loss 2.1956348419189453\n",
      "Epoch 7 Batch 480 Loss 1.941396713256836\n",
      "Epoch 7 Batch 490 Loss 1.824310302734375\n",
      "Epoch 7 Batch 500 Loss 1.9950649738311768\n",
      "Epoch 7 Batch 510 Loss 2.068638563156128\n",
      "Epoch 7 Batch 520 Loss 2.0756607055664062\n",
      "Epoch 7 Batch 530 Loss 2.087751626968384\n",
      "Epoch 7 Batch 540 Loss 2.269545316696167\n",
      "Epoch 7 Batch 550 Loss 2.1151387691497803\n",
      "Epoch 7 Batch 560 Loss 1.8970813751220703\n",
      "Epoch 7 Batch 570 Loss 1.970923662185669\n",
      "Epoch 7 Batch 580 Loss 2.0913331508636475\n",
      "Epoch 7 Batch 590 Loss 2.069920539855957\n",
      "Epoch 7 Batch 600 Loss 2.013277053833008\n",
      "Epoch 7 Batch 610 Loss 2.0423574447631836\n",
      "Epoch 7 Batch 620 Loss 2.059309244155884\n",
      "Epoch 7 Batch 630 Loss 2.205792188644409\n",
      "Epoch 7 Batch 640 Loss 2.044335126876831\n",
      "Epoch 7 Batch 650 Loss 2.0741326808929443\n",
      "Epoch 7 Batch 660 Loss 2.1115500926971436\n",
      "Epoch 7 Batch 670 Loss 2.314532518386841\n",
      "Epoch 7 Batch 680 Loss 1.969734787940979\n",
      "Epoch 7 Batch 690 Loss 1.9503024816513062\n",
      "Epoch 7 Batch 700 Loss 2.1804616451263428\n",
      "Epoch 7 Batch 710 Loss 2.2275474071502686\n",
      "Epoch 7 Batch 720 Loss 1.904766321182251\n",
      "Epoch 7 Batch 730 Loss 1.951155662536621\n",
      "Epoch 7 Batch 740 Loss 2.030268669128418\n",
      "Epoch 7 Batch 750 Loss 1.8918250799179077\n",
      "Epoch 7 Batch 760 Loss 2.037116289138794\n",
      "Epoch 7 Batch 770 Loss 2.053598165512085\n",
      "Epoch 7 Batch 780 Loss 2.202857732772827\n",
      "Epoch 7 Batch 790 Loss 2.1692750453948975\n",
      "Epoch 7 Batch 800 Loss 1.834672212600708\n",
      "Epoch 7 Batch 810 Loss 2.1141014099121094\n",
      "Epoch 7 Batch 820 Loss 2.3127248287200928\n",
      "Epoch 7 Batch 830 Loss 2.239624500274658\n",
      "Epoch 7 Batch 840 Loss 1.9429620504379272\n",
      "Epoch 7 Batch 850 Loss 2.1125147342681885\n",
      "Epoch 7 Batch 860 Loss 2.1290252208709717\n",
      "Epoch 7 Batch 870 Loss 1.931322693824768\n",
      "Epoch 7 Batch 880 Loss 2.317199230194092\n",
      "Epoch 7 Batch 890 Loss 2.1613686084747314\n",
      "Epoch 7 Batch 900 Loss 2.048967123031616\n",
      "Epoch 7 Batch 910 Loss 2.013089418411255\n",
      "Epoch 7 Batch 920 Loss 2.3815758228302\n",
      "Epoch 7 Batch 930 Loss 2.234039068222046\n",
      "Epoch 7 Batch 940 Loss 2.1820132732391357\n",
      "Epoch 7 Batch 950 Loss 2.0249216556549072\n",
      "Epoch 7 Batch 960 Loss 1.940260887145996\n",
      "Epoch 7 Batch 970 Loss 2.2268600463867188\n",
      "Epoch 7 Batch 980 Loss 2.001995325088501\n",
      "Epoch 7 Batch 990 Loss 2.3406543731689453\n",
      "Epoch 7 Batch 1000 Loss 2.2985196113586426\n",
      "Epoch 7 Batch 1010 Loss 2.242255926132202\n",
      "Epoch 7 Batch 1020 Loss 1.9352340698242188\n",
      "Epoch 7 Batch 1030 Loss 1.8127775192260742\n",
      "Epoch 7 Loss 2.0688\n",
      "Epoch 8 Batch 0 Loss 1.7803553342819214\n",
      "Epoch 8 Batch 10 Loss 2.0053937435150146\n",
      "Epoch 8 Batch 20 Loss 1.845139741897583\n",
      "Epoch 8 Batch 30 Loss 1.7324813604354858\n",
      "Epoch 8 Batch 40 Loss 2.021467924118042\n",
      "Epoch 8 Batch 50 Loss 2.182619571685791\n",
      "Epoch 8 Batch 60 Loss 1.6758537292480469\n",
      "Epoch 8 Batch 70 Loss 2.2234928607940674\n",
      "Epoch 8 Batch 80 Loss 1.7635082006454468\n",
      "Epoch 8 Batch 90 Loss 1.995977759361267\n",
      "Epoch 8 Batch 100 Loss 1.9508857727050781\n",
      "Epoch 8 Batch 110 Loss 1.946004867553711\n",
      "Epoch 8 Batch 120 Loss 1.8456875085830688\n",
      "Epoch 8 Batch 130 Loss 1.883713722229004\n",
      "Epoch 8 Batch 140 Loss 1.8696117401123047\n",
      "Epoch 8 Batch 150 Loss 1.9617117643356323\n",
      "Epoch 8 Batch 160 Loss 1.9391634464263916\n",
      "Epoch 8 Batch 170 Loss 1.8068088293075562\n",
      "Epoch 8 Batch 180 Loss 1.9735100269317627\n",
      "Epoch 8 Batch 190 Loss 1.9943777322769165\n",
      "Epoch 8 Batch 200 Loss 2.261596441268921\n",
      "Epoch 8 Batch 210 Loss 1.9955246448516846\n",
      "Epoch 8 Batch 220 Loss 1.9593470096588135\n",
      "Epoch 8 Batch 230 Loss 1.8685935735702515\n",
      "Epoch 8 Batch 240 Loss 1.8299540281295776\n",
      "Epoch 8 Batch 250 Loss 1.8823957443237305\n",
      "Epoch 8 Batch 260 Loss 1.946717619895935\n",
      "Epoch 8 Batch 270 Loss 2.0119597911834717\n",
      "Epoch 8 Batch 280 Loss 2.004011869430542\n",
      "Epoch 8 Batch 290 Loss 2.121784210205078\n",
      "Epoch 8 Batch 300 Loss 2.086874008178711\n",
      "Epoch 8 Batch 310 Loss 1.8340080976486206\n",
      "Epoch 8 Batch 320 Loss 1.9506067037582397\n",
      "Epoch 8 Batch 330 Loss 2.051750659942627\n",
      "Epoch 8 Batch 340 Loss 2.051654815673828\n",
      "Epoch 8 Batch 350 Loss 1.863684058189392\n",
      "Epoch 8 Batch 360 Loss 2.0825066566467285\n",
      "Epoch 8 Batch 370 Loss 1.7967042922973633\n",
      "Epoch 8 Batch 380 Loss 2.0316169261932373\n",
      "Epoch 8 Batch 390 Loss 1.9195774793624878\n",
      "Epoch 8 Batch 400 Loss 1.9133656024932861\n",
      "Epoch 8 Batch 410 Loss 1.9425852298736572\n",
      "Epoch 8 Batch 420 Loss 2.3678207397460938\n",
      "Epoch 8 Batch 430 Loss 1.7831528186798096\n",
      "Epoch 8 Batch 440 Loss 2.189063310623169\n",
      "Epoch 8 Batch 450 Loss 1.880812644958496\n",
      "Epoch 8 Batch 460 Loss 2.1398351192474365\n",
      "Epoch 8 Batch 470 Loss 1.7337827682495117\n",
      "Epoch 8 Batch 480 Loss 1.9904955625534058\n",
      "Epoch 8 Batch 490 Loss 1.9965709447860718\n",
      "Epoch 8 Batch 500 Loss 2.006129264831543\n",
      "Epoch 8 Batch 510 Loss 1.9077221155166626\n",
      "Epoch 8 Batch 520 Loss 2.076037645339966\n",
      "Epoch 8 Batch 530 Loss 1.9989982843399048\n",
      "Epoch 8 Batch 540 Loss 1.9976863861083984\n",
      "Epoch 8 Batch 550 Loss 1.9874759912490845\n",
      "Epoch 8 Batch 560 Loss 2.13679575920105\n",
      "Epoch 8 Batch 570 Loss 2.189293622970581\n",
      "Epoch 8 Batch 580 Loss 1.962648630142212\n",
      "Epoch 8 Batch 590 Loss 1.9061673879623413\n",
      "Epoch 8 Batch 600 Loss 2.300503730773926\n",
      "Epoch 8 Batch 610 Loss 2.018036365509033\n",
      "Epoch 8 Batch 620 Loss 2.042346715927124\n",
      "Epoch 8 Batch 630 Loss 2.0191848278045654\n",
      "Epoch 8 Batch 640 Loss 2.075563669204712\n",
      "Epoch 8 Batch 650 Loss 1.8840233087539673\n",
      "Epoch 8 Batch 660 Loss 1.8769419193267822\n",
      "Epoch 8 Batch 670 Loss 2.321315050125122\n",
      "Epoch 8 Batch 680 Loss 2.18654727935791\n",
      "Epoch 8 Batch 690 Loss 2.0166726112365723\n",
      "Epoch 8 Batch 700 Loss 2.072404146194458\n",
      "Epoch 8 Batch 710 Loss 2.0502212047576904\n",
      "Epoch 8 Batch 720 Loss 2.0620315074920654\n",
      "Epoch 8 Batch 730 Loss 1.8887819051742554\n",
      "Epoch 8 Batch 740 Loss 2.0294060707092285\n",
      "Epoch 8 Batch 750 Loss 2.0615203380584717\n",
      "Epoch 8 Batch 760 Loss 2.073009490966797\n",
      "Epoch 8 Batch 770 Loss 2.2249255180358887\n",
      "Epoch 8 Batch 780 Loss 2.004011869430542\n",
      "Epoch 8 Batch 790 Loss 2.269225835800171\n",
      "Epoch 8 Batch 800 Loss 1.9707187414169312\n",
      "Epoch 8 Batch 810 Loss 2.116330862045288\n",
      "Epoch 8 Batch 820 Loss 2.1229472160339355\n",
      "Epoch 8 Batch 830 Loss 2.0742619037628174\n",
      "Epoch 8 Batch 840 Loss 1.8904560804367065\n",
      "Epoch 8 Batch 850 Loss 1.7628878355026245\n",
      "Epoch 8 Batch 860 Loss 2.215967893600464\n",
      "Epoch 8 Batch 870 Loss 1.8690340518951416\n",
      "Epoch 8 Batch 880 Loss 2.146944284439087\n",
      "Epoch 8 Batch 890 Loss 2.043485164642334\n",
      "Epoch 8 Batch 900 Loss 1.975990653038025\n",
      "Epoch 8 Batch 910 Loss 1.8196245431900024\n",
      "Epoch 8 Batch 920 Loss 1.989465355873108\n",
      "Epoch 8 Batch 930 Loss 1.8640282154083252\n",
      "Epoch 8 Batch 940 Loss 1.8369911909103394\n",
      "Epoch 8 Batch 950 Loss 2.18462872505188\n",
      "Epoch 8 Batch 960 Loss 1.9488238096237183\n",
      "Epoch 8 Batch 970 Loss 2.1100919246673584\n",
      "Epoch 8 Batch 980 Loss 2.0489978790283203\n",
      "Epoch 8 Batch 990 Loss 1.8330557346343994\n",
      "Epoch 8 Batch 1000 Loss 1.9157081842422485\n",
      "Epoch 8 Batch 1010 Loss 2.2071101665496826\n",
      "Epoch 8 Batch 1020 Loss 2.1713101863861084\n",
      "Epoch 8 Batch 1030 Loss 2.072178602218628\n",
      "Epoch 8 Loss 1.9959\n",
      "Epoch 9 Batch 0 Loss 1.9119662046432495\n",
      "Epoch 9 Batch 10 Loss 2.004207134246826\n",
      "Epoch 9 Batch 20 Loss 2.254951238632202\n",
      "Epoch 9 Batch 30 Loss 1.8969039916992188\n",
      "Epoch 9 Batch 40 Loss 1.8749796152114868\n",
      "Epoch 9 Batch 50 Loss 1.9457085132598877\n",
      "Epoch 9 Batch 60 Loss 1.7384220361709595\n",
      "Epoch 9 Batch 70 Loss 2.0073130130767822\n",
      "Epoch 9 Batch 80 Loss 1.7535995244979858\n",
      "Epoch 9 Batch 90 Loss 1.8506653308868408\n",
      "Epoch 9 Batch 100 Loss 1.9902805089950562\n",
      "Epoch 9 Batch 110 Loss 1.7613470554351807\n",
      "Epoch 9 Batch 120 Loss 1.8580188751220703\n",
      "Epoch 9 Batch 130 Loss 1.9033339023590088\n",
      "Epoch 9 Batch 140 Loss 1.898714303970337\n",
      "Epoch 9 Batch 150 Loss 2.0660464763641357\n",
      "Epoch 9 Batch 160 Loss 1.9050136804580688\n",
      "Epoch 9 Batch 170 Loss 1.8718498945236206\n",
      "Epoch 9 Batch 180 Loss 1.945675253868103\n",
      "Epoch 9 Batch 190 Loss 2.0354104042053223\n",
      "Epoch 9 Batch 200 Loss 1.9820377826690674\n",
      "Epoch 9 Batch 210 Loss 1.955142617225647\n",
      "Epoch 9 Batch 220 Loss 1.8365811109542847\n",
      "Epoch 9 Batch 230 Loss 2.0201902389526367\n",
      "Epoch 9 Batch 240 Loss 2.1073853969573975\n",
      "Epoch 9 Batch 250 Loss 2.034163236618042\n",
      "Epoch 9 Batch 260 Loss 1.6849712133407593\n",
      "Epoch 9 Batch 270 Loss 2.060781955718994\n",
      "Epoch 9 Batch 280 Loss 1.9900035858154297\n",
      "Epoch 9 Batch 290 Loss 1.8342418670654297\n",
      "Epoch 9 Batch 300 Loss 1.945507287979126\n",
      "Epoch 9 Batch 310 Loss 1.7452694177627563\n",
      "Epoch 9 Batch 320 Loss 1.875797152519226\n",
      "Epoch 9 Batch 330 Loss 1.8522144556045532\n",
      "Epoch 9 Batch 340 Loss 1.891344428062439\n",
      "Epoch 9 Batch 350 Loss 2.0961077213287354\n",
      "Epoch 9 Batch 360 Loss 1.75377357006073\n",
      "Epoch 9 Batch 370 Loss 1.906314730644226\n",
      "Epoch 9 Batch 380 Loss 1.8181819915771484\n",
      "Epoch 9 Batch 390 Loss 1.9077666997909546\n",
      "Epoch 9 Batch 400 Loss 1.932813048362732\n",
      "Epoch 9 Batch 410 Loss 2.000927448272705\n",
      "Epoch 9 Batch 420 Loss 1.914425253868103\n",
      "Epoch 9 Batch 430 Loss 2.0608673095703125\n",
      "Epoch 9 Batch 440 Loss 1.9092044830322266\n",
      "Epoch 9 Batch 450 Loss 2.0785000324249268\n",
      "Epoch 9 Batch 460 Loss 2.020134687423706\n",
      "Epoch 9 Batch 470 Loss 2.022792339324951\n",
      "Epoch 9 Batch 480 Loss 1.8391265869140625\n",
      "Epoch 9 Batch 490 Loss 1.80293869972229\n",
      "Epoch 9 Batch 500 Loss 1.8503427505493164\n",
      "Epoch 9 Batch 510 Loss 1.9693737030029297\n",
      "Epoch 9 Batch 520 Loss 1.868630290031433\n",
      "Epoch 9 Batch 530 Loss 1.9658981561660767\n",
      "Epoch 9 Batch 540 Loss 1.9326927661895752\n",
      "Epoch 9 Batch 550 Loss 1.8931255340576172\n",
      "Epoch 9 Batch 560 Loss 1.842799186706543\n",
      "Epoch 9 Batch 570 Loss 2.1263234615325928\n",
      "Epoch 9 Batch 580 Loss 1.8073517084121704\n",
      "Epoch 9 Batch 590 Loss 2.020590305328369\n",
      "Epoch 9 Batch 600 Loss 1.9255409240722656\n",
      "Epoch 9 Batch 610 Loss 1.9446899890899658\n",
      "Epoch 9 Batch 620 Loss 1.9889458417892456\n",
      "Epoch 9 Batch 630 Loss 2.0296223163604736\n",
      "Epoch 9 Batch 640 Loss 1.9249905347824097\n",
      "Epoch 9 Batch 650 Loss 2.0510966777801514\n",
      "Epoch 9 Batch 660 Loss 1.888562798500061\n",
      "Epoch 9 Batch 670 Loss 1.834625244140625\n",
      "Epoch 9 Batch 680 Loss 1.9026821851730347\n",
      "Epoch 9 Batch 690 Loss 1.6020516157150269\n",
      "Epoch 9 Batch 700 Loss 2.0072717666625977\n",
      "Epoch 9 Batch 710 Loss 2.0129287242889404\n",
      "Epoch 9 Batch 720 Loss 1.813975214958191\n",
      "Epoch 9 Batch 730 Loss 1.9249016046524048\n",
      "Epoch 9 Batch 740 Loss 1.9164680242538452\n",
      "Epoch 9 Batch 750 Loss 1.8428033590316772\n",
      "Epoch 9 Batch 760 Loss 1.895994782447815\n",
      "Epoch 9 Batch 770 Loss 1.9956457614898682\n",
      "Epoch 9 Batch 780 Loss 1.8478904962539673\n",
      "Epoch 9 Batch 790 Loss 1.9106483459472656\n",
      "Epoch 9 Batch 800 Loss 1.7893993854522705\n",
      "Epoch 9 Batch 810 Loss 1.9323654174804688\n",
      "Epoch 9 Batch 820 Loss 1.9664452075958252\n",
      "Epoch 9 Batch 830 Loss 1.8792043924331665\n",
      "Epoch 9 Batch 840 Loss 1.7687901258468628\n",
      "Epoch 9 Batch 850 Loss 2.064432382583618\n",
      "Epoch 9 Batch 860 Loss 1.8880881071090698\n",
      "Epoch 9 Batch 870 Loss 1.9942299127578735\n",
      "Epoch 9 Batch 880 Loss 1.9443947076797485\n",
      "Epoch 9 Batch 890 Loss 1.846574068069458\n",
      "Epoch 9 Batch 900 Loss 1.9880497455596924\n",
      "Epoch 9 Batch 910 Loss 2.1057584285736084\n",
      "Epoch 9 Batch 920 Loss 2.0413010120391846\n",
      "Epoch 9 Batch 930 Loss 1.9986928701400757\n",
      "Epoch 9 Batch 940 Loss 1.7642558813095093\n",
      "Epoch 9 Batch 950 Loss 1.8948156833648682\n",
      "Epoch 9 Batch 960 Loss 2.0219485759735107\n",
      "Epoch 9 Batch 970 Loss 1.935012698173523\n",
      "Epoch 9 Batch 980 Loss 1.8764488697052002\n",
      "Epoch 9 Batch 990 Loss 1.7687605619430542\n",
      "Epoch 9 Batch 1000 Loss 1.7975167036056519\n",
      "Epoch 9 Batch 1010 Loss 1.927669882774353\n",
      "Epoch 9 Batch 1020 Loss 1.8347828388214111\n",
      "Epoch 9 Batch 1030 Loss 2.0069470405578613\n",
      "Epoch 9 Loss 1.9252\n",
      "Epoch 10 Batch 0 Loss 1.7156429290771484\n",
      "Epoch 10 Batch 10 Loss 1.8727577924728394\n",
      "Epoch 10 Batch 20 Loss 1.7737751007080078\n",
      "Epoch 10 Batch 30 Loss 1.7356747388839722\n",
      "Epoch 10 Batch 40 Loss 1.749285101890564\n",
      "Epoch 10 Batch 50 Loss 1.8271293640136719\n",
      "Epoch 10 Batch 60 Loss 1.767317771911621\n",
      "Epoch 10 Batch 70 Loss 1.9632691144943237\n",
      "Epoch 10 Batch 80 Loss 1.9672555923461914\n",
      "Epoch 10 Batch 90 Loss 1.7157843112945557\n",
      "Epoch 10 Batch 100 Loss 1.8985334634780884\n",
      "Epoch 10 Batch 110 Loss 1.7306574583053589\n",
      "Epoch 10 Batch 120 Loss 1.5511919260025024\n",
      "Epoch 10 Batch 130 Loss 1.8234003782272339\n",
      "Epoch 10 Batch 140 Loss 1.827720046043396\n",
      "Epoch 10 Batch 150 Loss 1.9233198165893555\n",
      "Epoch 10 Batch 160 Loss 1.7849358320236206\n",
      "Epoch 10 Batch 170 Loss 1.9265869855880737\n",
      "Epoch 10 Batch 180 Loss 1.6922123432159424\n",
      "Epoch 10 Batch 190 Loss 1.930506944656372\n",
      "Epoch 10 Batch 200 Loss 1.6800310611724854\n",
      "Epoch 10 Batch 210 Loss 1.8052330017089844\n",
      "Epoch 10 Batch 220 Loss 1.9290493726730347\n",
      "Epoch 10 Batch 230 Loss 1.7222307920455933\n",
      "Epoch 10 Batch 240 Loss 1.9243751764297485\n",
      "Epoch 10 Batch 250 Loss 1.8147071599960327\n",
      "Epoch 10 Batch 260 Loss 1.6851717233657837\n",
      "Epoch 10 Batch 270 Loss 1.875088095664978\n",
      "Epoch 10 Batch 280 Loss 1.6308425664901733\n",
      "Epoch 10 Batch 290 Loss 1.8108888864517212\n",
      "Epoch 10 Batch 300 Loss 1.8975744247436523\n",
      "Epoch 10 Batch 310 Loss 1.695431113243103\n",
      "Epoch 10 Batch 320 Loss 1.9072927236557007\n",
      "Epoch 10 Batch 330 Loss 1.8007668256759644\n",
      "Epoch 10 Batch 340 Loss 1.6417360305786133\n",
      "Epoch 10 Batch 350 Loss 1.986336588859558\n",
      "Epoch 10 Batch 360 Loss 1.986307144165039\n",
      "Epoch 10 Batch 370 Loss 1.642943024635315\n",
      "Epoch 10 Batch 380 Loss 1.9414604902267456\n",
      "Epoch 10 Batch 390 Loss 1.793627142906189\n",
      "Epoch 10 Batch 400 Loss 1.9984782934188843\n",
      "Epoch 10 Batch 410 Loss 1.8536843061447144\n",
      "Epoch 10 Batch 420 Loss 1.7981586456298828\n",
      "Epoch 10 Batch 430 Loss 1.6876776218414307\n",
      "Epoch 10 Batch 440 Loss 1.7844011783599854\n",
      "Epoch 10 Batch 450 Loss 1.9883149862289429\n",
      "Epoch 10 Batch 460 Loss 1.793023705482483\n",
      "Epoch 10 Batch 470 Loss 1.741754174232483\n",
      "Epoch 10 Batch 480 Loss 2.046725034713745\n",
      "Epoch 10 Batch 490 Loss 1.7132107019424438\n",
      "Epoch 10 Batch 500 Loss 1.8842023611068726\n",
      "Epoch 10 Batch 510 Loss 1.6397026777267456\n",
      "Epoch 10 Batch 520 Loss 1.869010329246521\n",
      "Epoch 10 Batch 530 Loss 1.7946033477783203\n",
      "Epoch 10 Batch 540 Loss 1.886881709098816\n",
      "Epoch 10 Batch 550 Loss 2.0189342498779297\n",
      "Epoch 10 Batch 560 Loss 1.7428592443466187\n",
      "Epoch 10 Batch 570 Loss 1.7503674030303955\n",
      "Epoch 10 Batch 580 Loss 1.7720507383346558\n",
      "Epoch 10 Batch 590 Loss 1.7426722049713135\n",
      "Epoch 10 Batch 600 Loss 2.0644967555999756\n",
      "Epoch 10 Batch 610 Loss 1.867401123046875\n",
      "Epoch 10 Batch 620 Loss 2.0010838508605957\n",
      "Epoch 10 Batch 630 Loss 1.9539026021957397\n",
      "Epoch 10 Batch 640 Loss 1.8158520460128784\n",
      "Epoch 10 Batch 650 Loss 1.5231993198394775\n",
      "Epoch 10 Batch 660 Loss 1.9303505420684814\n",
      "Epoch 10 Batch 670 Loss 1.9176052808761597\n",
      "Epoch 10 Batch 680 Loss 2.0884506702423096\n",
      "Epoch 10 Batch 690 Loss 1.9234853982925415\n",
      "Epoch 10 Batch 700 Loss 1.7030365467071533\n",
      "Epoch 10 Batch 710 Loss 1.8064113855361938\n",
      "Epoch 10 Batch 720 Loss 2.0130367279052734\n",
      "Epoch 10 Batch 730 Loss 1.7980823516845703\n",
      "Epoch 10 Batch 740 Loss 1.9520387649536133\n",
      "Epoch 10 Batch 750 Loss 2.0508923530578613\n",
      "Epoch 10 Batch 760 Loss 2.1477062702178955\n",
      "Epoch 10 Batch 770 Loss 1.6791342496871948\n",
      "Epoch 10 Batch 780 Loss 1.684417724609375\n",
      "Epoch 10 Batch 790 Loss 1.8384201526641846\n",
      "Epoch 10 Batch 800 Loss 1.80241858959198\n",
      "Epoch 10 Batch 810 Loss 1.6728709936141968\n",
      "Epoch 10 Batch 820 Loss 1.6604083776474\n",
      "Epoch 10 Batch 830 Loss 1.9323879480361938\n",
      "Epoch 10 Batch 840 Loss 1.864755630493164\n",
      "Epoch 10 Batch 850 Loss 1.803597331047058\n",
      "Epoch 10 Batch 860 Loss 1.7585972547531128\n",
      "Epoch 10 Batch 870 Loss 1.8043241500854492\n",
      "Epoch 10 Batch 880 Loss 1.8215082883834839\n",
      "Epoch 10 Batch 890 Loss 1.6142033338546753\n",
      "Epoch 10 Batch 900 Loss 1.7874640226364136\n",
      "Epoch 10 Batch 910 Loss 1.621531367301941\n",
      "Epoch 10 Batch 920 Loss 1.7383899688720703\n",
      "Epoch 10 Batch 930 Loss 1.9980610609054565\n",
      "Epoch 10 Batch 940 Loss 1.860163927078247\n",
      "Epoch 10 Batch 950 Loss 1.7941468954086304\n",
      "Epoch 10 Batch 960 Loss 1.7442200183868408\n",
      "Epoch 10 Batch 970 Loss 1.9765818119049072\n",
      "Epoch 10 Batch 980 Loss 1.7653559446334839\n",
      "Epoch 10 Batch 990 Loss 1.7776364088058472\n",
      "Epoch 10 Batch 1000 Loss 1.8242601156234741\n",
      "Epoch 10 Batch 1010 Loss 1.602272629737854\n",
      "Epoch 10 Batch 1020 Loss 1.9347666501998901\n",
      "Epoch 10 Batch 1030 Loss 1.8013015985488892\n",
      "Epoch 10 Loss 1.8551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련 데이터 분리\n",
    "dataset = tf.data.Dataset.from_tensor_slices((kor_tensor, eng_tensor))\n",
    "dataset = dataset.shuffle(len(kor_tensor)).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "# 에포크 설정\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    enc_hidden = [tf.zeros((batch_size, units)) for _ in range(2)]\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy()}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {total_loss / batch:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Output: obama has been a candidate\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Output: the <unk>\n",
      "Input: 커피는 필요 없다.\n",
      "Output: its not going to be a lot of the world\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Output: the us military service\n"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = preprocess_kor(sentence)\n",
    "    inputs = [kor_tokenizer.word_index.get(\n",
    "        word, kor_tokenizer.word_index['<unk>']) for word in sentence]\n",
    "    inputs = pad_sequences(\n",
    "        [inputs], maxlen=kor_tensor.shape[1], padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = []\n",
    "    hidden = tf.zeros((1, 512))  # 초기 hidden state\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inputs, [hidden, hidden])\n",
    "\n",
    "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(40):  # 최대 번역 길이\n",
    "        predictions, dec_hidden_h, dec_hidden_c, _ = decoder(\n",
    "            dec_input, [enc_hidden_h, enc_hidden_c], enc_output)\n",
    "        predicted_id = tf.argmax(predictions[0][0]).numpy()\n",
    "        if eng_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "        result.append(eng_tokenizer.index_word[predicted_id])\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "# 평가\n",
    "test_sentences = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Output: {evaluate(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
