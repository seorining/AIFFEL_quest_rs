{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1e15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a092617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 읽기\n",
    "file_path = './data/ChatbotData.csv'\n",
    "chatbot_data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터 확인\n",
    "print(chatbot_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08a6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = chatbot_data['Q'].tolist()\n",
    "answer = chatbot_data['A'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff28d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  11823\n",
      ">> 12시 땡!\n",
      ">> 가스비 비싼데 감기 걸리겠어\n",
      ">> 간만에 떨리니까 좋더라\n",
      ">> 감정컨트롤을 못하겠어\n",
      ">> 개강룩 입어볼까\n",
      "answer:  11823\n",
      ">> 하루가 또 가네요.\n",
      ">> 따뜻하게 사세요!\n",
      ">> 떨리는 감정은 그 자체로 소중해요.\n",
      ">> 그건 습관이에요.\n",
      ">> 개시해보세요.\n"
     ]
    }
   ],
   "source": [
    "print('question: ', len(question))\n",
    "for sen in question[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print('answer: ', len(answer))\n",
    "for sen in answer[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8e821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 1. 모든 영문자를 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. 영문자, 한글, 숫자, 주요 특수문자만 남기고 나머지 제거, 탭문자 살리기\n",
    "    sentence = re.sub(r\"[^a-z0-9ㄱ-ㅎ가-힣.,!?()\\t]\", \" \", sentence)\n",
    "\n",
    "    # 3. 둘 이상의 공백을 하나의 공백으로 치환\n",
    "    sentence = re.sub(r\"\\s{2,}\", \" \", sentence).strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a107f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = list(map(preprocess_sentence, question))\n",
    "answer = list(map(preprocess_sentence, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4e7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  118\n",
      "\n",
      "\n",
      "Train question: 11705\n",
      ">> 12시 땡!\n",
      ">> 가스비 비싼데 감기 걸리겠어\n",
      ">> 간만에 떨리니까 좋더라\n",
      ">> 감정컨트롤을 못하겠어\n",
      ">> 개강룩 입어볼까\n",
      "\n",
      "\n",
      "Train answer: 11705\n",
      ">> 하루가 또 가네요.\n",
      ">> 따뜻하게 사세요!\n",
      ">> 떨리는 감정은 그 자체로 소중해요.\n",
      ">> 그건 습관이에요.\n",
      ">> 개시해보세요.\n",
      "\n",
      "\n",
      "Test question: 118\n",
      ">> 친구의 구남친을 좋아하게 됐어요.\n",
      ">> 카페 알바생이 좋아졌는데 들이대도 괜찮을까?\n",
      ">> 키 큰 여자는 별로인가?\n",
      ">> 학교에 심남 있는데 연락해볼까?\n",
      ">> 헤어지고 얼마 안됐는데 썸타는 거 가능?\n",
      "\n",
      "\n",
      "Test answer: 118\n",
      ">> 선택을 해야 겠네요.\n",
      ">> 언제 끝나냐고 물어보세요.\n",
      ">> 키는 중요하지 않을 거예요.\n",
      ">> 호감을 어느 정도 표현해보는 것도 좋을 것 같아요.\n",
      ">> 썸 정도는 언제 타든 상관 없는 거 같아요.\n"
     ]
    }
   ],
   "source": [
    "total_sentence_count = len(question)\n",
    "test_sentence_count = total_sentence_count // 100\n",
    "\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "train_question = question[:-test_sentence_count]\n",
    "train_answer = answer[:-test_sentence_count]\n",
    "\n",
    "test_question = question[-test_sentence_count:]\n",
    "test_answer = answer[-test_sentence_count:]\n",
    "\n",
    "print(\"Train question:\", len(train_question))\n",
    "for sen in train_question[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "print(\"Train answer:\", len(train_answer))\n",
    "for sen in train_answer[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")   \n",
    "print(\"Test question:\", len(test_question))\n",
    "for sen in test_question[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "print(\"Test answer:\", len(test_answer))\n",
    "for sen in test_answer[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370d294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab 객체 생성\n",
    "mecab = Mecab()\n",
    "\n",
    "def build_corpus(src_data, tgt_data, tokenizer, max_len=50):\n",
    "    \"\"\"\n",
    "    소스 및 타겟 문장 데이터를 전처리하고 토큰화하여 중복을 제거한 코퍼스 반환\n",
    "    - src_data: 소스 문장 리스트\n",
    "    - tgt_data: 타겟 문장 리스트\n",
    "    - tokenizer: 토크나이즈 함수 (예: mecab.morphs)\n",
    "    - max_len: 최대 토큰 길이\n",
    "    \"\"\"\n",
    "    src_corpus, tgt_corpus = [], []\n",
    "    # 중복 방지용 세트\n",
    "    unique_src = set()\n",
    "    unique_tgt = set()\n",
    "\n",
    "    for src_sentence, tgt_sentence in zip(src_data, tgt_data):\n",
    "        # 1. 문장 정제\n",
    "        src_sentence = preprocess_sentence(src_sentence)\n",
    "        tgt_sentence = preprocess_sentence(tgt_sentence)\n",
    "\n",
    "        # 2. 토큰화\n",
    "        src_tokens = tokenizer(src_sentence)\n",
    "        tgt_tokens = tokenizer(tgt_sentence)\n",
    "\n",
    "        # 3. 토큰 길이 검사\n",
    "        if len(src_tokens) > max_len or len(tgt_tokens) > max_len:\n",
    "            continue  # 길이 초과 문장은 제외\n",
    "\n",
    "        # 4. 중복 체크 후 저장\n",
    "        src_duplicated = tuple(src_tokens)\n",
    "        tgt_duplicated = tuple(tgt_tokens)\n",
    "            \n",
    "        if src_duplicated not in unique_src and tgt_duplicated not in unique_tgt:\n",
    "            src_corpus.append(src_tokens)\n",
    "            tgt_corpus.append(tgt_tokens)\n",
    "                \n",
    "            unique_src.add(src_duplicated)\n",
    "            unique_tgt.add(tgt_duplicated)\n",
    "\n",
    "    return src_corpus, tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b7e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'ppl 심하네']\n",
      "\n",
      "['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.']\n"
     ]
    }
   ],
   "source": [
    "train_question, train_answer = build_corpus(train_question, train_answer, mecab.morphs)\n",
    "\n",
    "print(question[:5])\n",
    "print()\n",
    "print(answer[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55665c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Word2Vec 모델 로드\n",
    "embedding_model_path = './data/ko.bin'  # 다운로드한 ko.bin 파일 경로\n",
    "\n",
    "wv = Word2Vec.load(embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07193036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, wv):\n",
    "    # 문장에서 무작위로 단어 선택\n",
    "    selected_tok = random.choice(sentence)\n",
    "    \n",
    "    # 유사한 단어로 변환된 결과 문장을 저장할 변수\n",
    "    result = []\n",
    "    \n",
    "    for tok in sentence:\n",
    "        if tok == selected_tok:\n",
    "            try:\n",
    "                # 선택된 단어와 유사한 단어로 대체\n",
    "                similar_word = wv.most_similar(tok)[0][0]\n",
    "                result.append(similar_word)\n",
    "            except KeyError:\n",
    "                # 단어가 임베딩 모델에 없을 경우 원래 단어 사용\n",
    "                result.append(tok)\n",
    "        else:\n",
    "            result.append(tok)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2fa3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강 함수\n",
    "def augment_data(que_corpus, ans_corpus, wv):\n",
    "    augmented_que_corpus = []\n",
    "    augmented_ans_corpus = []\n",
    "\n",
    "    # 1. 질문 문장에 Lexical Substitution 적용\n",
    "    for que_sentence in tqdm(que_corpus, desc=\"Augmenting Questions\"):\n",
    "        augmented_que_corpus.append(lexical_sub(que_sentence, wv))\n",
    "\n",
    "\n",
    "    # 2. 답변 문장에 Lexical Substitution 적용\n",
    "    for ans_sentence in tqdm(ans_corpus, desc=\"Augmenting Answers\"):\n",
    "        augmented_ans_corpus.append(lexical_sub(ans_sentence, wv))\n",
    "\n",
    "\n",
    "    # 3. 병렬 데이터셋 구성 (총 3배 데이터)\n",
    "    final_que_corpus = que_corpus + augmented_que_corpus + que_corpus\n",
    "    final_ans_corpus = ans_corpus + ans_corpus + augmented_ans_corpus\n",
    "\n",
    "    return final_que_corpus, final_ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3d43d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a290fdef24be4c67a7ec9ec0f94c45dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting Questions:   0%|          | 0/7594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_453/4244547378.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  similar_word = wv.most_similar(tok)[0][0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8b797c99534b018c6644cc25655f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting Answers:   0%|          | 0/7594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "증강 후 질문 데이터 개수: 22782\n",
      "증강 후 답변 데이터 개수: 22782\n",
      "예시 증강 데이터:\n",
      "질문: [['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['ppl', '심하', '네'], ['sd', '카드', '망가졌', '어']]\n",
      "답변: [['하루', '가', '또', '가', '네요', '.'], ['위로', '해', '드립니다', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['눈살', '이', '찌푸려', '지', '죠', '.'], ['다시', '새로', '사', '는', '게', '마음', '편해요', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 증강\n",
    "augmented_que_corpus, augmented_ans_corpus = augment_data(train_question, train_answer, wv)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"증강 후 질문 데이터 개수:\", len(augmented_que_corpus))\n",
    "print(\"증강 후 답변 데이터 개수:\", len(augmented_ans_corpus))\n",
    "print(\"예시 증강 데이터:\")\n",
    "print(\"질문:\", augmented_que_corpus[:5])\n",
    "print(\"답변:\", augmented_ans_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b85358d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 데이터에 <start>와 <end> 토큰 추가\n",
    "ans_question = [[\"<start>\"] + corpus + [\"<end>\"] for corpus in augmented_que_corpus]\n",
    "ans_answer = [[\"<start>\"] + corpus + [\"<end>\"] for corpus in augmented_ans_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28323ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 추가 후 ans_question: [['<start>', '12', '시', '땡', '!', '<end>'], ['<start>', '1', '지망', '학교', '떨어졌', '어', '<end>'], ['<start>', '3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다', '<end>'], ['<start>', 'ppl', '심하', '네', '<end>'], ['<start>', 'sd', '카드', '망가졌', '어', '<end>']]\n",
      "토큰 추가 후 ans_answer: [['<start>', '하루', '가', '또', '가', '네요', '.', '<end>'], ['<start>', '위로', '해', '드립니다', '.', '<end>'], ['<start>', '여행', '은', '언제나', '좋', '죠', '.', '<end>'], ['<start>', '눈살', '이', '찌푸려', '지', '죠', '.', '<end>'], ['<start>', '다시', '새로', '사', '는', '게', '마음', '편해요', '.', '<end>']]\n"
     ]
    }
   ],
   "source": [
    "print(\"토큰 추가 후 ans_question:\", ans_question[:5])\n",
    "print(\"토큰 추가 후 ans_answer:\", ans_answer[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e548b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ko_tokenize(tokenized_corpus, VOCA_SIZE):\n",
    "   \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCA_SIZE, filters='')\n",
    "    tokenizer.fit_on_texts(tokenized_corpus)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def vectorize(tokenized_corpus, tokenizer):\n",
    "    tensor = tokenizer.texts_to_sequences(tokenized_corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=50)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "857b7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenizer = ko_tokenize(ans_question + ans_answer, VOCAB_SIZE)\n",
    "enc_train = vectorize(ans_question, tokenizer)\n",
    "dec_train = vectorize(ans_answer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63cddeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22782, 50)\n",
      "(22782, 50)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train.shape)\n",
    "print(dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cb0de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc35fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc3b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a847385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07ee8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac7ca48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad0b1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3434af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a7b3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd81d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b319026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=1,\n",
    "    d_model=368,\n",
    "    n_heads=8,\n",
    "    d_ff=1024,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.2,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model=368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53b46492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)  # d_model을 float32로 변환\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)  # warmup_steps을 float32로 변환\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)  # step도 float32로 변환\n",
    "        \n",
    "        # 모든 지수 연산에 대해 tf.constant()를 사용하여 명시적 타입 지정\n",
    "        arg1 = tf.math.pow(step, tf.constant(-0.5, dtype=tf.float32))\n",
    "        arg2 = step * tf.math.pow(self.warmup_steps, tf.constant(-1.5, dtype=tf.float32))\n",
    "        \n",
    "        return tf.math.pow(self.d_model, tf.constant(-0.5, dtype=tf.float32)) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b7aba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cf217f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edb2302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1f68dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f286fb57f043cba34e6feea55f95ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.3047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e112bf5b25e84f868ea658c2aa2c5f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 3.8588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87306a0f4e5644849d8da4167ece234f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 3.2788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240f8aec0c824e9f90c93e3353daa83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2.8477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a4c31885ff4916a6734a2b270e6b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 2.3135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeefc0ac1b84ceeb770ce2412bad98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1.6401\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4187ba6806400982c71e826006855d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 1.1064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49fe038b15c4ae0a923a9a1a0f33e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.7714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d24a3f51654848a6eec5f7b266f69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.5874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13252d26e92f4ee48b60bf22a0626e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.5074\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count, desc=f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "\n",
    "    # 배치 단위로 학습\n",
    "    for batch, (inputs, targets) in enumerate(train_dataset):\n",
    "        loss, _, _, _ = train_step(inputs, targets, transformer, optimizer)\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        # 진행 상황 업데이트\n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_postfix(loss=float(loss))\n",
    "\n",
    "    tqdm_bar.close()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / dataset_count:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8abfa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_texts(sequences, tokenizer):\n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    return [\" \".join([reverse_word_map.get(i, \"\") for i in seq]) for seq in sequences]\n",
    "\n",
    "\n",
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.word_index['<start>']], 0)  \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.word_index['<end>'] == predicted_id:\n",
    "            result = sequences_to_texts(ids, tgt_tokenizer)\n",
    "            return result\n",
    "\n",
    "        ids.append([predicted_id])\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    \n",
    "    result = sequences_to_texts(ids, tgt_tokenizer)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35dc5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    src_tokens = src_tokenizer.texts_to_sequences(src_sentence)\n",
    "    tgt_tokens = tgt_tokenizer.texts_to_sequences(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence\n",
    "    candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad016c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49d49827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5ffa8adf6e4f7f94655733b9fcdab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 118\n",
      "Total Score: 0.003889679875584514\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "eval_bleu(transformer, test_question, test_answer, tokenizer, tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98781862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
