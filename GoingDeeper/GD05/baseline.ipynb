{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 20:18:17.463189: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-20 20:18:17.481570: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-20 20:18:17.481588: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-20 20:18:17.481601: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-20 20:18:17.485207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n",
      "1.26.0\n",
      "3.9.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\" 이번 김 위원장의 방중은 딕 체니 미국 부통령이 베이징을 방문, 중국에 북한의 핵 위협 완화를 요구한 직후 이루어졌다.\tKim's visit comes just after Vice President Dick Cheney traveled to Beijing and urged China to do more to defuse Pyongyang's nuclear threat.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.getcwd()\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f:\n",
    "        kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f:\n",
    "        eng = f.read().splitlines()\n",
    "    \n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    clean_set = set()\n",
    "    for k, e in zip(kor, eng):\n",
    "        clean_set.add(k + '\\t' + e)\n",
    "    cleaned_corpus = sorted(list(clean_set))\n",
    "    \n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)\n",
    "\n",
    "print(cleaned_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번 김 위원장의 방중은 딕 체니 미국 부통령이 베이징을 방문 , 중국에 북한의 핵 위협 완화를 요구한 직후 이루어졌다 .\n",
      "kim's visit comes just after vice president dick cheney traveled to beijing and urged china to do more to defuse pyongyang's nuclear threat .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 1. 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    # 2. 알파벳, 문장부호, 한글만 남기고 모두 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣.,!?']\", \" \", sentence)\n",
    "    # 3. 문장부호 양옆에 공백 추가\n",
    "    sentence = re.sub(r\"([.,!?])\", r\" \\1 \", sentence)\n",
    "    # 4. 여러 공백을 하나의 공백으로 변환\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    # 5. 문장 앞뒤의 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "for sentence in cleaned_corpus[0].split('\\t'):\n",
    "    print(preprocess_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/workspace/aiffel/transformer/korean-english-park.train.ko --model_prefix=ko_spm --vocab_size=20000--pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /workspace/aiffel/transformer/korean-english-park.train.ko\n",
      "  input_format: \n",
      "  model_prefix: ko_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /workspace/aiffel/transformer/korean-english-park.train.ko\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 94123 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=5811421\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=1324\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 94123 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=2558685\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 176917 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 94123\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 241207\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 241207 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=95194 obj=14.8536 num_tokens=531054 num_tokens/piece=5.57865\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=83620 obj=13.5153 num_tokens=533454 num_tokens/piece=6.3795\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=62708 obj=13.5651 num_tokens=555794 num_tokens/piece=8.86321\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=62675 obj=13.5242 num_tokens=556110 num_tokens/piece=8.87292\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=47005 obj=13.7065 num_tokens=585127 num_tokens/piece=12.4482\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=47005 obj=13.6666 num_tokens=585283 num_tokens/piece=12.4515\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35253 obj=13.8983 num_tokens=616933 num_tokens/piece=17.5002\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=35253 obj=13.8487 num_tokens=616918 num_tokens/piece=17.4997\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=26439 obj=14.13 num_tokens=649516 num_tokens/piece=24.5666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=26439 obj=14.0747 num_tokens=649501 num_tokens/piece=24.566\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22000 obj=14.2776 num_tokens=670847 num_tokens/piece=30.493\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22000 obj=14.2403 num_tokens=670845 num_tokens/piece=30.493\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ko_spm.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ko_spm.vocab\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/workspace/aiffel/transformer/korean-english-park.train.en --model_prefix=en_spm --vocab_size=20000--pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /workspace/aiffel/transformer/korean-english-park.train.en\n",
      "  input_format: \n",
      "  model_prefix: en_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /workspace/aiffel/transformer/korean-english-park.train.en\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 94123 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=11967043\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.955% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=82\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99955\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 94123 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=6785919\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 142424 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 94123\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 109507\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 109507 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=55814 obj=11.4619 num_tokens=233542 num_tokens/piece=4.18429\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=47034 obj=9.15535 num_tokens=234338 num_tokens/piece=4.98231\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35271 obj=9.13866 num_tokens=247538 num_tokens/piece=7.01817\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=35261 obj=9.12083 num_tokens=247655 num_tokens/piece=7.02348\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=26445 obj=9.21377 num_tokens=268037 num_tokens/piece=10.1356\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=26445 obj=9.19031 num_tokens=268024 num_tokens/piece=10.1351\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22000 obj=9.26777 num_tokens=282496 num_tokens/piece=12.8407\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22000 obj=9.25198 num_tokens=282491 num_tokens/piece=12.8405\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: en_spm.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: en_spm.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다\n",
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size=20000,\n",
    "                       lang=\"ko\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    \n",
    "    if lang == 'ko':\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            f'--input={kor_path} --model_prefix={lang}_spm --vocab_size={vocab_size}'\n",
    "            f'--pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}'\n",
    "        )\n",
    "    elif lang == 'en':\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            f'--input={eng_path} --model_prefix={lang}_spm --vocab_size={vocab_size}'\n",
    "            f'--pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}'\n",
    "        )\n",
    "    else: \n",
    "        print('lang 오류')\n",
    "        \n",
    "    # 학습된 모델 로드\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f\"{lang}_spm.model\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair.split(\"\\t\")\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO Tokenizer 예제: ['▁안', '녕', '하세요', ',', '▁만나', '서', '▁반', '갑', '습니다', '.']\n",
      "EN Tokenizer 예제: ['<s>', '▁', 'Hello', ',', '▁nice', '▁to', '▁meet', '▁you', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Test 결과 확인\n",
    "print(\"KO Tokenizer 예제:\", ko_tokenizer.EncodeAsPieces(\"안녕하세요, 만나서 반갑습니다.\"))\n",
    "print(\"EN Tokenizer 예제:\", en_tokenizer.EncodeAsPieces(\"Hello, nice to meet you.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09db36d70a794fbd842d8703d72830f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 78968 중 64679개의 문장이 선택되었습니다.\n",
      "enc_train shape: (64679, 50)\n",
      "dec_train shape: (64679, 50)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "# 한국어와 영어 코퍼스의 길이 확인\n",
    "assert len(kor_corpus) == len(eng_corpus), \"한국어와 영어 코퍼스의 길이가 일치하지 않습니다.\"\n",
    "\n",
    "# 토큰 길이가 50 이하인 문장만 선별\n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    # 한국어와 영어 문장을 각각 토큰화\n",
    "    src_tokens = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    tgt_tokens = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "\n",
    "    # 토큰 길이가 50 이하인 경우만 저장\n",
    "    if len(src_tokens) <= 50 and len(tgt_tokens) <= 50:\n",
    "        src_corpus.append(src_tokens)\n",
    "        tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "# 패딩 처리를 완료하여 학습용 데이터를 생성\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tgt_corpus, padding='post')\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"전체 데이터 {len(kor_corpus)} 중 {len(src_corpus)}개의 문장이 선택되었습니다.\")\n",
    "print(f\"enc_train shape: {enc_train.shape}\")\n",
    "print(f\"dec_train shape: {dec_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   53,   739,  3811,     6, 16370,    13,  4211,  2045,    49,\n",
       "        8777,  1011,     4,   501,     5,    12,  4370,   103,     6,\n",
       "         285,   856,  2453,     7,  3034,  2542,  4648,     5,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    14,  3757,   104,    12,     6,   430,   866,   161,\n",
       "          46,  1483,   181,  2852,  3119,    14,  2800,  5797,   703,\n",
       "          23,     7,    32, 13051,    20,    11,    14,  2055, 11015,\n",
       "         136,     7,   183,    60,     7, 11603,   484,  6310,  7808,\n",
       "          12,     6,   140,  1122,    14,     4,     2,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i)\n",
    "                              for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)\n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights\n",
    "    \n",
    "\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn\n",
    "    \n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(\n",
    "            out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "    \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(\n",
    "            d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns\n",
    "    \n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                           for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "                self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "    \n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "            self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 20:18:28.155644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.158075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.158142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.159121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.159172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.159208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.209411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.209481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.209538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 20:18:28.209580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22260 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "NUM_LAYERS = 2            # Transformer layers\n",
    "D_MODEL = 512             # Embedding dimension\n",
    "NUM_HEADS = 8             # Number of attention heads\n",
    "D_FF = 2048               # Feed forward network hidden layer size\n",
    "SRC_VOCAB_SIZE = 20000     # Source vocabulary size (한국어)\n",
    "TGT_VOCAB_SIZE = 20000     # Target vocabulary size (영어)\n",
    "MAX_SEQ_LEN = 100         # Maximum sequence length\n",
    "DROPOUT = 0.3             # Dropout rate\n",
    "\n",
    "# Transformer 모델 선언\n",
    "transformer = Transformer(\n",
    "    n_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=NUM_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    pos_len=MAX_SEQ_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    shared=False  # False로 설정해 독립적인 Embedding 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)  # step을 부동소수점으로 캐스팅\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    패딩 마스크를 생성합니다.\n",
    "    seq에서 0(PAD)인 위치를 1로, 나머지를 0으로 설정합니다.\n",
    "    \"\"\"\n",
    "    return tf.cast(tf.math.equal(seq, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def generate_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    룩어헤드 마스크를 생성합니다.\n",
    "    디코더에서 현재와 이전의 토큰만 볼 수 있도록 상삼각 행렬을 생성합니다.\n",
    "    \"\"\"\n",
    "    return 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    \"\"\"\n",
    "    Transformer 모델에서 필요한 마스크들을 생성합니다.\n",
    "    Args:\n",
    "        src (Tensor): 소스 입력 텐서\n",
    "        tgt (Tensor): 타겟 입력 텐서\n",
    "    Returns:\n",
    "        enc_mask: 인코더 입력의 패딩 마스크\n",
    "        dec_enc_mask: 디코더가 인코더 출력과 상호작용할 때 사용하는 패딩 마스크\n",
    "        dec_mask: 디코더의 룩어헤드 + 패딩 마스크\n",
    "    \"\"\"\n",
    "    # 인코더 패딩 마스크\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 디코더 패딩 마스크\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 디코더 룩어헤드 + 패딩 마스크\n",
    "    seq_len = tf.shape(tgt)[1]\n",
    "    look_ahead_mask = generate_look_ahead_mask(seq_len)\n",
    "    dec_pad_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(look_ahead_mask, dec_pad_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    # 타겟 데이터에서 <BOS> 제거 (gold는 정답)\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    # 입력 마스크 생성\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # GradientTape으로 손실 계산\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "            src, tgt, enc_mask, dec_enc_mask, dec_mask\n",
    "        )\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 그래디언트 계산 및 가중치 업데이트\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data,\n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0,\n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src),\n",
    "                 :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt),\n",
    "                 :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(\n",
    "                tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "            generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "            model(_input,\n",
    "                  output,\n",
    "                  enc_padding_mask,\n",
    "                  combined_mask,\n",
    "                  dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "            tf.argmax(tf.math.softmax(predictions, axis=-1)\n",
    "                      [0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat(\n",
    "            [output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(),\n",
    "                            enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67575/2869332712.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  t = tqdm_notebook(idx_list)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2308e6664005488488f71e97c033c0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 20:18:31.681052: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0cc3a27ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-01-20 20:18:31.681065: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-01-20 20:18:31.683293: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-20 20:18:31.689883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2025-01-20 20:18:31.732249: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: i , , .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: i , , .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i , , .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: i , , .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac25694349b94e81b717f3331580a82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: i's , , , , , .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: i's a u . s .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i's a u . s .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: i's a , , , , , , , , , , , , .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31e50bd098248b298cd3954f838dc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president of korea has said\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: i'm , i am .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: he's been a lot of .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a year in apr .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dde115bf0024c3e9679f2e90a7d684c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has urged u . s .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: it's a little .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i think i want to go up .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a total of u . s .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50897d1b6e8a450e81b5c6b8b8b8f95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: it is a bit of americans .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: but it's a good problem .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a total of people killed people were killed in a suicide bomber .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bc3bfca8f042e4bcee617f48b2c2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: cnn . . . . .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: it's a very big .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it's a good .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: more than , people have been killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0284f3e77f884f789658ff9037518542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: bush , i know , i know .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: and , , , , , , , , , , .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: . clean up your sleep\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: four people were killed in a week .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fba53761034497abe391e9fb6736a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: roeny , , is virtually , rollan , ross , rogge .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they're live in a town of kashtun .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: . . . . . . . . . . . . . . . . . .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least people were killed and were injured .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb78bbf810b4b8bb7a97dbeb4bc87a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: i pledge to him to go to presidential polls .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they have their supporters .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: they need to cut your brain\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab18b900cbeb4e759072b0dc4f0f5458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama , whom obama cnn\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they live in a river .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: . clean doesn't eat\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980a1ac97c614526a0df2aa1332193fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president lee , john mccain , .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they are cities in cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there's a sense of fun\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: six people were killed in a week .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e64d264dfba4e849078d9649939520d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama holds a blue presidential nomination .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they have a strong battle .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: they don't stop\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a dozen people were killed in .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b224c7798e34987907364f1029ae4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: jon , jon , jon .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: a large building of a town of rural spirits .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i've always asked anyone to make\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a third person was killed in a week on a . m .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11f126ffd9a4dc0a3e3d0c82f06715a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: joe mccain , .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: their cities are large .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i'm going to start .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a1c37e630a4d0f8e4a9c64d9f3d496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama , i will make a john , former president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people are cities of their homes .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i don't need to get more .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: six people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b76c5edee94bada05e0273639697a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: i answer obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people threaten threatens to cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i don't need to make money .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a bomb was killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc45440bfd74a6fafcae6836a20346e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama holds .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people are cities along with a forest .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i don't have a sense .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a third person was killed in a qaso .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ec635894e84b8a83d968a0f5c8e57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: joan mccain .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people threaten their property line .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: can't pay about you much more caffeine a year .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a third person was killed in a town about , people died .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be55fea3cea4787990dac63e00d39ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: i finish him .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people targeted cities in cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i've been a joke .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: six people died monday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d85338ad494fbfa52dfd792eda2beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: lee will take place .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: sometimes drifted out of town .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: cnn you need you much more .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a half dozen people were killed .\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                       dec_train[idx:idx+BATCH_SIZE],\n",
    "                       transformer,\n",
    "                       optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
